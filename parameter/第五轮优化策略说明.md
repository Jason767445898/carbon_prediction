# 第五轮参数调优策略说明

**调优时间**: 2025-10-15  
**预计时长**: 3小时  
**实验配置数**: 12组

---

## 📊 当前状态分析（基于2025-10-15 00:53报告）

### 模型性能概览
| 模型 | R² | RMSE | MAE | MAPE | 方向准确率 | 状态 |
|------|-----|------|-----|------|-----------|------|
| **RandomForest** | 0.9290 | 45.44 | 33.52 | 3.18% | 60.24% | ✅ 优秀 |
| **GradientBoosting** | 0.8742 | 60.47 | 43.30 | 4.04% | 55.82% | ✅ 优秀 |
| **Transformer** | 0.7746 | 49.19 | 42.61 | 4.47% | 55.56% | ✅ 良好 |
| **XGBoost** | 0.6603 | 99.38 | 58.99 | 4.99% | 51.81% | ⚠️ 良好 |
| **LSTM** | 0.5740 | 67.63 | 54.61 | 5.67% | 44.97% | ❌ 待改进 |

### 🚨 关键问题
1. **LSTM严重退化**: 从第四轮的R²=0.8904降至0.574（下降35.5%）
2. **主配置文件可能被覆盖**: 需要恢复第四轮最佳配置
3. **Transformer保持稳定**: 从0.7874微降至0.7746，简化策略有效

---

## 🎯 第五轮优化目标

### 主要目标
1. **LSTM恢复**: R² > 0.89（重现第四轮最佳水平）
2. **Transformer稳定**: R² ≥ 0.77 或突破至 R² > 0.80
3. **终极目标**: LSTM R² > 0.92, Transformer R² > 0.85

### 成功标准
- ✅ **完全达标**: LSTM R²≥0.89 且 Transformer R²≥0.77
- ⚠️ **部分达标**: LSTM R²≥0.85 或 Transformer R²≥0.75
- ❌ **需继续优化**: 两者均未达到上述标准

---

## 🔬 优化策略

### 核心原则
1. **围绕成功配置微调**: 以第四轮最佳配置为基准，±10%范围内调整
2. **避免大幅跳跃**: 每次只调整1-2个参数
3. **系统性探索**: 覆盖epochs、dropout、units、batch_size等维度

### 参数调整策略

#### LSTM优化维度
| 维度 | 第四轮最佳 | 调整范围 | 策略 |
|------|-----------|---------|------|
| units | [72, 36] | [64-80, 32-40] | ±10%微调 + 三层探索 |
| dropout | 0.16 | 0.14-0.18 | 小步长调整 |
| epochs | 140 | 140-160 | 延长训练 |
| batch_size | 8 | 4-8 | 极小batch测试 |

#### Transformer优化维度
| 维度 | 第四轮最佳 | 调整范围 | 策略 |
|------|-----------|---------|------|
| d_model | 16 | 16-24 | 容量扩展 |
| num_heads | 2 | 2 | 保持稳定 |
| num_layers | 2 | 2-3 | 深度探索 |
| dropout | 0.6 | 0.55-0.65 | 平衡过拟合 |
| epochs | 100 | 100-120 | 延长训练 |

---

## 📋 12组实验配置详情

### 第一组：基线恢复 (3组)

#### 配置1: 第四轮最佳配置基线
```python
LSTM: units=[72,36], dropout=0.16, epochs=140, batch_size=8
Transformer: d_model=16, num_heads=2, num_layers=2, dff=64, dropout=0.6, epochs=100
```
**目的**: 验证第四轮最佳配置是否可复现

#### 配置2: LSTM延长训练+10%
```python
LSTM: units=[72,36], dropout=0.16, epochs=154, batch_size=8  # epochs+10%
Transformer: 保持配置1不变
```
**目的**: 测试更长训练是否有助于收敛

#### 配置3: LSTM降低dropout-10%
```python
LSTM: units=[72,36], dropout=0.14, epochs=140, batch_size=8  # dropout-10%
Transformer: 保持配置1不变
```
**目的**: 减少正则化，增强拟合能力

---

### 第二组：容量调整 (3组)

#### 配置4: LSTM增加神经元+10%
```python
LSTM: units=[80,40], dropout=0.16, epochs=140, batch_size=8  # units+10%
Transformer: 保持配置1不变
```
**目的**: 测试更大网络容量

#### 配置5: Transformer扩容d_model+25%
```python
LSTM: 保持配置1不变
Transformer: d_model=20, num_heads=2, num_layers=2, dff=80, dropout=0.6, epochs=100
```
**目的**: 增强Transformer表示能力

#### 配置6: Transformer降低dropout-0.05
```python
LSTM: 保持配置1不变
Transformer: d_model=16, num_heads=2, num_layers=2, dff=64, dropout=0.55, epochs=100
```
**目的**: 平衡过拟合与欠拟合

---

### 第三组：联合优化 (3组)

#### 配置7: 联合增加训练轮数
```python
LSTM: units=[72,36], dropout=0.16, epochs=160, batch_size=8
Transformer: d_model=16, num_heads=2, num_layers=2, dff=64, dropout=0.6, epochs=120
```
**目的**: 双模型同时延长训练

#### 配置8: 联合极小batch_size=4
```python
LSTM: units=[72,36], dropout=0.16, epochs=140, batch_size=4
Transformer: d_model=16, num_heads=2, num_layers=2, dff=64, dropout=0.6, epochs=100, batch_size=4
```
**目的**: 更细粒度的梯度更新

#### 配置9: LSTM增加深度三层
```python
LSTM: units=[72,48,24], dropout=0.16, epochs=140, batch_size=8  # 三层网络
Transformer: 保持配置1不变
```
**目的**: 探索更深层次的时序特征

---

### 第四组：极致优化 (3组)

#### 配置10: Transformer增加深度三层
```python
LSTM: 保持配置1不变
Transformer: d_model=16, num_heads=2, num_layers=3, dff=64, dropout=0.65, epochs=100
```
**目的**: 深层Transformer + 高dropout防止过拟合

#### 配置11: LSTM极致优化-多维增强
```python
LSTM: units=[80,40], dropout=0.14, epochs=160, batch_size=8  # 三维同时优化
Transformer: 保持配置1不变
```
**目的**: 容量+训练+正则化三维强化

#### 配置12: Transformer极致优化-突破R²0.80
```python
LSTM: 保持配置1不变
Transformer: d_model=24, num_heads=2, num_layers=2, dff=96, dropout=0.55, epochs=120
```
**目的**: 大幅扩容 + 降dropout + 延长训练

---

## ⏱️ 时间预估

| 阶段 | 配置数 | 单配置耗时 | 小计 |
|------|--------|-----------|------|
| 基线恢复 | 3 | 12分钟 | 36分钟 |
| 容量调整 | 3 | 12分钟 | 36分钟 |
| 联合优化 | 3 | 15分钟 | 45分钟 |
| 极致优化 | 3 | 15分钟 | 45分钟 |
| **总计** | **12** | - | **约162分钟（2.7小时）** |

**预留缓冲时间**: 18分钟  
**总预计时长**: **3小时**

---

## 📈 预期成果

### 最佳情况（概率30%）
- LSTM: R² ≥ 0.92, RMSE < 32
- Transformer: R² ≥ 0.85, RMSE < 45

### 理想情况（概率50%）
- LSTM: R² ≥ 0.89, RMSE < 35
- Transformer: R² ≥ 0.77, RMSE < 50

### 保守情况（概率20%）
- LSTM: R² ≥ 0.85, RMSE < 40
- Transformer: R² ≥ 0.75, RMSE < 52

---

## 🔍 关键监控指标

1. **R²变化趋势**: 每轮记录，绘制曲线
2. **RMSE稳定性**: 避免过拟合导致的波动
3. **MAPE实用性**: 关注相对误差
4. **方向准确率**: 交易策略的关键指标

---

## 📝 执行流程

```bash
cd /Users/Jason/Desktop/code/AI/parameter
python3 parameter_tuning.py
```

### 实时监控
- 每完成一组配置，立即记录到 `parameter_tuning.txt`
- 动态更新最佳配置
- 如有异常，立即中止并分析

---

## 🎓 经验总结

### 第四轮成功经验
1. ✅ **batch_size=8** 是LSTM的黄金配置
2. ✅ **极简Transformer**（d_model=16, 2层）有效防止过拟合
3. ✅ **dropout=0.6** 对Transformer至关重要
4. ✅ **units=[72,36]** 是LSTM的最佳容量

### 本轮改进重点
1. 🎯 严格复现第四轮最佳配置
2. 🎯 微调而非大幅跳跃
3. 🎯 联合优化双模型
4. 🎯 探索深度网络可能性

---

## ⚠️ 风险提示

1. **过拟合风险**: 极端配置可能导致训练集表现好但泛化差
2. **训练时间**: 深层网络或大batch可能超时
3. **配置冲突**: 某些参数组合可能不兼容
4. **随机性**: TensorFlow随机种子影响，建议多次运行

---

**制定人**: AI Assistant  
**执行时间**: 2025-10-15  
**文档版本**: v5.0
