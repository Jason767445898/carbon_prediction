# 第三轮参数优化策略说明

## 📊 优化背景

### 第二轮最佳结果回顾
| 模型 | R² | RMSE | MAE | MAPE | 状态 |
|------|-----|------|-----|------|------|
| **LSTM** | **0.8768** | **36.37** | **25.82** | **2.68%** | ✅ **优秀** |
| Transformer | -0.9251 | 143.78 | 131.36 | 14.37% | ❌ 严重过拟合 |
| RandomForest | 0.9430 | 40.69 | 29.58 | 2.81% | 🏆 基准 |

### 关键发现
1. **LSTM突破**: batch_size=8时性能显著提升(R²从0.6778→0.8768)
2. **Transformer困境**: 所有配置R²均为负,严重过拟合
3. **优化空间**: LSTM与RandomForest仍有0.066的R²差距

---

## 🎯 第三轮优化目标

### LSTM模型目标
- **当前**: R²=0.8768, RMSE=36.37
- **目标**: R²>0.90, RMSE<35
- **策略**: 基于batch_size=8的最佳配置微调

### Transformer模型目标
- **当前**: R²<0 (所有配置均失败)
- **目标**: R²>0.3, RMSE<150
- **策略**: 激进简化,防止过拟合

---

## 🔧 优化策略详解

### LSTM优化策略(12个配置)

#### 1. 基线验证
```python
{'units': [64, 32], 'dropout': 0.2, 'epochs': 100, 'batch_size': 8}
```
- 验证第二轮最佳配置的稳定性

#### 2. Dropout调优(配置2-3)
```python
# 降低dropout,提高拟合能力
{'units': [64, 32], 'dropout': 0.15, 'batch_size': 8}
{'units': [64, 32], 'dropout': 0.1, 'batch_size': 8}
```
**理由**: 当前dropout=0.2可能过度正则化,限制了性能提升

#### 3. 训练轮数优化(配置4)
```python
{'units': [64, 32], 'dropout': 0.2, 'epochs': 150, 'batch_size': 8}
```
**理由**: 增加训练轮数,让模型充分学习

#### 4. 网络宽度探索(配置5-6)
```python
{'units': [96, 48], 'dropout': 0.2, 'batch_size': 8}
{'units': [128, 64], 'dropout': 0.2, 'batch_size': 8}
```
**理由**: 适度增加网络容量,提升表达能力

#### 5. 网络深度探索(配置7, 12)
```python
{'units': [96, 64, 32], 'dropout': 0.2, 'batch_size': 8}
{'units': [128, 96, 64, 32], 'dropout': 0.2, 'batch_size': 8}
```
**理由**: 更深的网络可能捕捉更复杂的模式

#### 6. 组合优化(配置8-9, 11)
```python
# 低dropout + 更多epochs
{'units': [64, 32], 'dropout': 0.15, 'epochs': 150, 'batch_size': 8}

# 宽网络 + 低dropout
{'units': [96, 48], 'dropout': 0.15, 'epochs': 120, 'batch_size': 8}

# 平衡配置
{'units': [80, 40], 'dropout': 0.18, 'epochs': 120, 'batch_size': 8}
```
**理由**: 多维度组合寻找最优平衡点

#### 7. 极限测试(配置10)
```python
{'units': [64, 32], 'dropout': 0.2, 'epochs': 120, 'batch_size': 4}
```
**理由**: 测试更小batch_size的极限效果

---

### Transformer优化策略(8个配置)

#### 核心思想: **激进简化 + 极高正则化**

根据记忆知识:
> "当Transformer模型出现R²为负的严重过拟合时,应采用激进的简化策略:降低d_model维度、减少网络层数、增加dropout正则化"

#### 1. 极简单层配置(配置1-2)
```python
# 最激进简化
{'d_model': 16, 'num_heads': 2, 'num_layers': 1, 'dff': 64, 'dropout': 0.6}

# 超轻量级
{'d_model': 32, 'num_heads': 2, 'num_layers': 1, 'dff': 128, 'dropout': 0.5}
```
**参数量对比**:
- 原配置(d_model=256, 4层): ~2.6M参数 → R²=-7.9
- 新配置(d_model=16, 1层): ~5K参数 (减少99.8%)

#### 2. 长训练补偿(配置3)
```python
{'d_model': 24, 'num_heads': 2, 'num_layers': 1, 'dff': 96, 'dropout': 0.5, 'epochs': 150}
```
**理由**: 简化模型后,可以安全地增加训练轮数

#### 3. 最小可行配置(配置4)
```python
{'d_model': 16, 'num_heads': 4, 'num_layers': 1, 'dff': 64, 'dropout': 0.5}
```
**理由**: 测试最小可行的Transformer架构

#### 4. 小batch训练(配置5)
```python
{'d_model': 32, 'num_heads': 2, 'num_layers': 1, 'dff': 128, 'dropout': 0.5, 'batch_size': 8}
```
**理由**: 借鉴LSTM的成功经验,使用batch_size=8

#### 5. 两层极简(配置6)
```python
{'d_model': 16, 'num_heads': 2, 'num_layers': 2, 'dff': 64, 'dropout': 0.6}
```
**理由**: 在极简基础上增加一层,测试深度效果

#### 6. 平衡配置(配置7-8)
```python
{'d_model': 32, 'num_heads': 4, 'num_layers': 1, 'dff': 128, 'dropout': 0.4}
{'d_model': 24, 'num_heads': 4, 'num_layers': 1, 'dff': 96, 'dropout': 0.5}
```
**理由**: 在极简和性能之间寻找平衡

---

## 📈 预期改进路径

### LSTM模型预期
```
第一轮最佳: R²=0.6778 (batch_size=16)
    ↓ 优化batch_size
第二轮最佳: R²=0.8768 (batch_size=8)
    ↓ 微调网络结构+dropout
第三轮目标: R²>0.90
    ↓ 继续优化
最终目标: R²≈0.94 (接近RandomForest)
```

### Transformer模型预期
```
第一轮: R²=-7.93 (严重过拟合)
    ↓ 简化模型
第二轮: R²=-0.93 (仍过拟合)
    ↓ 激进简化
第三轮目标: R²>0.3
    ↓ 如达成
第四轮: R²>0.6 (可用水平)
```

---

## 🔍 技术改进点

### 1. 代码层面
✅ **已修复**: Transformer训练时batch_size从config读取
```python
# 修改前
batch_size=32  # 硬编码

# 修改后
batch_size=self.config['transformer_config'].get('batch_size', 32)
```

### 2. 数据预处理
✅ **已改进**: 自动移除全NaN列,使用多层插值
```
原始: 2494个NaN → 0填充
改进: 移除无效列 → 多层插值 → 0个NaN
```

### 3. 参数传递验证
✅ **已验证**: 所有配置参数正确传递到训练流程

---

## ⏱️ 预计执行时间

### LSTM部分(12个配置)
- 每个配置: ~5分钟
- 总计: ~60分钟

### Transformer部分(8个配置)  
- 每个配置: ~2-3分钟(简化后更快)
- 总计: ~20分钟

### 总执行时间
约 **80-90分钟**

---

## 📝 成功标准

### LSTM成功标准
- [ ] 至少1个配置达到R²>0.90
- [ ] RMSE<35
- [ ] MAPE<2.5%
- [ ] 训练稳定,无明显过拟合

### Transformer成功标准
- [ ] 至少1个配置达到R²>0
- [ ] R²>0.3为理想
- [ ] RMSE<150
- [ ] 验证集loss收敛

### 整体成功标准
- [ ] LSTM性能接近或超过RandomForest
- [ ] Transformer找到可用配置
- [ ] 完整记录所有实验结果
- [ ] 生成详细对比分析报告

---

## 📊 结果记录

所有结果将自动记录到:
- `parameter_tuning_v3.txt` - 详细参数和指标
- `parameter_tuning_v3_runtime.log` - 运行时日志

---

## 🎓 经验总结(待补充)

优化完成后将总结:
1. 最有效的参数调优策略
2. LSTM vs Transformer在小数据集上的表现
3. batch_size对模型性能的影响规律
4. dropout正则化的最佳实践
5. 网络深度vs宽度的权衡

---

**优化开始时间**: 2025-10-14 22:22  
**预计完成时间**: 2025-10-14 23:45  
**当前状态**: 🟢 运行中...
