# 碳价格预测模型参数优化说明

## 📊 第一轮调优结果分析

### LSTM模型表现
| 配置 | batch_size | dropout | R² | RMSE | 状态 |
|------|------------|---------|-------|------|------|
| 配置1 | 32 | 0.2 | -0.6614 | 133.56 | ❌ 差 |
| 配置2 | 32 | 0.2 | -0.4681 | 125.55 | ❌ 差 |
| 配置3 | 32 | 0.3 | -0.4092 | 123.01 | ❌ 差 |
| **配置4** | **16** | **0.2** | **0.6778** | **58.82** | ✅ **最佳** |
| 配置5 | 32 | 0.2 | -1.7995 | 173.38 | ❌ 差 |

### Transformer模型表现
| 配置 | d_model | num_heads | num_layers | R² | RMSE | 状态 |
|------|---------|-----------|------------|-------|------|------|
| 配置1 | 128 | 8 | 4 | -3.4284 | 218.06 | ❌ 严重过拟合 |
| 配置2 | 256 | 8 | 4 | -0.7693 | 137.84 | ❌ 过拟合 |
| 配置3 | 128 | 16 | 4 | -2.4539 | 192.58 | ❌ 过拟合 |
| 配置4 | 128 | 8 | 6 | -7.3543 | 299.51 | ❌ 严重过拟合 |
| 配置5 | 128 | 8 | 4 | -4.9404 | 252.56 | ❌ 过拟合 |
| 配置6 | 128 | 8 | 4 | -3.9597 | 230.77 | ❌ 过拟合 |

### 传统机器学习模型（参考基准）
- **RandomForest**: R² = 0.943, RMSE = 40.69 ✅ 优秀
- **GradientBoosting**: R² = 0.876, RMSE = 60.13 ✅ 良好
- **XGBoost**: R² = 0.660, RMSE = 99.38 ✅ 可接受

---

## 🎯 关键发现与问题诊断

### 1. LSTM模型问题
- **小batch_size效果显著**: batch_size=16时R²从负值跃升至0.6778
- **大batch_size表现差**: batch_size=32时模型无法收敛
- **原因**: 数据集较小(1247样本)，小batch_size提供更频繁的梯度更新

### 2. Transformer模型问题
- **所有配置R²为负**: 表明模型预测比简单均值更差
- **严重过拟合**: 模型复杂度过高，参数量达2.6M+
- **数据不匹配**: Transformer需要更多数据，当前数据量不足

### 3. 数据预处理问题
```
警告: 仍有 2494 个NaN值，将全部用0填充
```
- NaN值处理策略可能影响序列模型性能
- 需要更智能的插值或填充策略

---

## 🚀 第二轮优化策略

### LSTM模型优化方向
基于最佳配置（batch_size=16, R²=0.6778）进行微调：

#### 优化维度1: 网络结构
```python
# 增加深度
{'units': [128, 64, 32], 'dropout': 0.2, 'batch_size': 16}

# 增加宽度
{'units': [128, 64], 'dropout': 0.2, 'batch_size': 16}

# 平衡配置
{'units': [96, 64, 32], 'dropout': 0.25, 'batch_size': 16}
```

#### 优化维度2: 正则化
```python
# 降低dropout（当前可能过度正则化）
{'units': [64, 32], 'dropout': 0.1, 'batch_size': 16}

# 调整dropout
{'units': [64, 32], 'dropout': 0.15, 'batch_size': 16}
```

#### 优化维度3: 训练策略
```python
# 增加训练轮数
{'units': [64, 32], 'dropout': 0.2, 'epochs': 150, 'batch_size': 16}

# 更小batch_size
{'units': [64, 32], 'dropout': 0.2, 'epochs': 100, 'batch_size': 8}

# 更小batch + 更多epochs
{'units': [64, 32], 'dropout': 0.2, 'epochs': 200, 'batch_size': 8}
```

### Transformer模型优化方向
**核心策略**: 大幅简化模型，防止过拟合

#### 优化维度1: 减少模型复杂度
```python
# 轻量级配置（参数量减少80%+）
{
    'd_model': 64,
    'num_heads': 4,
    'num_layers': 2,  # 从4层减至2层
    'dff': 256,       # 从512减至256
    'dropout': 0.3,   # 增加正则化
    'epochs': 50
}

# 超轻量级配置
{
    'd_model': 32,
    'num_heads': 4,
    'num_layers': 2,
    'dff': 128,
    'dropout': 0.3,
    'epochs': 50
}
```

#### 优化维度2: 增强正则化
```python
# 高dropout配置
{
    'd_model': 64,
    'num_heads': 4,
    'num_layers': 3,
    'dff': 256,
    'dropout': 0.4,  # 从0.1增至0.4
    'epochs': 50
}

# 极简配置 + 高正则化
{
    'd_model': 64,
    'num_heads': 8,
    'num_layers': 2,
    'dff': 256,
    'dropout': 0.5,
    'epochs': 50
}
```

#### 优化维度3: 最小化尝试
```python
# 最小可行配置
{
    'd_model': 32,
    'num_heads': 2,
    'num_layers': 2,
    'dff': 128,
    'dropout': 0.3,
    'epochs': 80
}

# 单层Transformer
{
    'd_model': 32,
    'num_heads': 4,
    'num_layers': 1,  # 只用1层
    'dff': 128,
    'dropout': 0.2,
    'epochs': 100
}
```

---

## 📈 预期改进目标

### LSTM模型
- **当前最佳**: R² = 0.6778, RMSE = 58.82
- **目标**: R² > 0.75, RMSE < 50
- **策略**: 微调网络结构和训练参数

### Transformer模型
- **当前状态**: 所有配置R² < 0（无法使用）
- **目标**: R² > 0, RMSE < 100（至少可用）
- **策略**: 大幅简化模型，防止过拟合

### 整体目标
- 找到至少一个深度学习模型接近RandomForest性能(R²=0.943)
- 为不同场景提供多种可选模型配置
- 记录详细的调优过程和参数影响

---

## 🔧 技术改进建议

### 1. 数据预处理优化
```python
# 当前问题
警告: 仍有 2494 个NaN值，将全部用0填充

# 建议改进
- 使用线性插值或多项式插值替代0填充
- 使用前向/后向填充的组合
- 考虑使用KNN插值
- 移除NaN比例过高的特征
```

### 2. 特征工程优化
- 添加更多时间序列特征（移动平均、趋势等）
- 特征重要性分析，移除冗余特征
- 考虑使用PCA降维

### 3. 训练策略优化
- 使用K折交叉验证
- 实施early stopping防止过拟合
- 尝试学习率调度策略
- 考虑使用ensemble方法

### 4. 评估指标扩展
当前指标: MSE, MAE, RMSE, R², MAPE, Direction_Accuracy
建议添加:
- Sharpe Ratio (投资性能)
- Max Drawdown (最大回撤)
- Hit Rate (方向预测准确率)

---

## 📝 执行计划

### 第二轮调优
✅ **正在执行**
- 10个LSTM配置测试
- 8个Transformer配置测试
- 预计耗时: 2-3小时

### 后续优化
1. **数据预处理改进** (如发现仍有改进空间)
2. **特征工程优化** (如模型性能达不到预期)
3. **集成学习** (结合多个模型的优势)
4. **超参数网格搜索** (精细调优)

---

## 📊 参数配置文件位置

- **调优脚本**: `parameter_tuning.py`
- **调优日志**: `parameter_tuning.txt`
- **主程序**: `carbon_price_prediction.py`

---

## ⏰ 更新时间

- 第一轮调优完成: 2025-10-14 18:52
- 第二轮调优开始: 2025-10-14 20:46
- 文档创建: 2025-10-14 20:52

---

## 📌 关键结论

1. **小batch_size是关键**: 对于小数据集，batch_size=8-16表现最佳
2. **Transformer不适合小数据集**: 需要大幅简化或考虑放弃
3. **传统ML模型优异**: RandomForest和GradientBoosting表现出色，应作为baseline
4. **数据质量很重要**: NaN值处理策略显著影响模型性能
