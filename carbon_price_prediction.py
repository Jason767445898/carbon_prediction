#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¢³ä»·æ ¼é¢„æµ‹ç»¼åˆåˆ†æç³»ç»Ÿ
æ•´åˆLSTMã€Transformer Attentionå’ŒSHAPå¯è§£é‡Šæ€§åˆ†æ

ä¸»è¦åŠŸèƒ½ï¼š
1. ä»Excelã€CSVæˆ–Stataæ–‡ä»¶åŠ è½½ç¢³ä»·æ ¼å’Œç›¸å…³å› å­æ•°æ®
2. ä½¿ç”¨LSTMå’ŒTransformeræ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹
3. é€šè¿‡SHAPåˆ†ææ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§
4. è¾“å‡ºé¢„æµ‹ç»“æœã€å‡†ç¡®åº¦è¯„ä¼°å’Œè§£é‡Šæ€§åˆ†ææŠ¥å‘Š

æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š
- Excelæ–‡ä»¶ (.xlsx, .xls)
- CSVæ–‡ä»¶ (.csv)
- Stataæ–‡ä»¶ (.dta)

ä½¿ç”¨è¯´æ˜ï¼šè¯¦è§ã€Šç¢³ä»·æ ¼é¢„æµ‹ç³»ç»Ÿä½¿ç”¨æŒ‡å—.mdã€‹å’Œã€ŠSTATA_USAGE_GUIDE.mdã€‹
"""

# =============================================================================
# å…¨å±€é…ç½®å’Œè·¯å¾„è®¾ç½®
# =============================================================================

# é»˜è®¤æ•°æ®æ–‡ä»¶è·¯å¾„
DEFAULT_DATA_FILE = 'data.dta'
SAMPLE_DATA_FILE = 'carbon_price_prediction_test_data.xlsx'

# è¾“å‡ºç›®å½•é…ç½®
OUTPUT_DIRS = {
    'txt': 'outputs/logs',
    'excel': 'outputs/reports', 
    'pic': 'outputs/visualizations'
}

# æ–‡ä»¶åæ ¼å¼é…ç½®
FILE_NAME_FORMAT = {
    'program_name': 'carbon_price_prediction',
    'timestamp_format': '%Y%m%d_%H%M%S'
}

# é»˜è®¤ç³»ç»Ÿé…ç½®
DEFAULT_CONFIG = {
    'target_column': 'coal_price',  
    'sequence_length': 60,
    'test_size': 0.2,
    'validation_size': 0.1,
    'random_state': 42,
    'lstm_config': {
        'units': [72, 36],
        'dropout': 0.16,
        'epochs': 140,
        'batch_size': 8
        },
    'transformer_config': {
        'd_model': 16,
        'num_heads': 2,
        'num_layers': 2,
        'dff': 64,
        'dropout': 0.6,
        'epochs': 100,
        'batch_size': 8
        }
}

# =============================================================================
# å¯¼å…¥å¿…è¦çš„åº“
# =============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import os
from datetime import datetime

# æ£€æŸ¥pandasç‰ˆæœ¬æ˜¯å¦æ”¯æŒStataæ–‡ä»¶è¯»å–
try:
    import pandas as pd
    # æ£€æŸ¥æ˜¯å¦æ”¯æŒread_stata
    PANDAS_STATA_SUPPORT = hasattr(pd, 'read_stata')
    if not PANDAS_STATA_SUPPORT:
        print("âš ï¸ å½“å‰pandasç‰ˆæœ¬ä¸æ”¯æŒè¯»å–Stataæ–‡ä»¶")
except ImportError:
    PANDAS_STATA_SUPPORT = False
    print("âš ï¸ pandasä¸å¯ç”¨ï¼Œå°†æ— æ³•è¯»å–Stataæ–‡ä»¶")

# æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ 
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# æ·±åº¦å­¦ä¹ åº“
try:
    import tensorflow as tf  # type: ignore
    # ç»Ÿä¸€ä½¿ç”¨tf.keras
    from tensorflow.keras import layers  # type: ignore
    from tensorflow.keras.models import Sequential  # type: ignore
    from tensorflow.keras.layers import LSTM, Dense, Dropout  # type: ignore
    from tensorflow.keras.optimizers import Adam  # type: ignore
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # type: ignore
    keras = tf.keras  # ä¸ºäº†å…¼å®¹æ€§
    TENSORFLOW_AVAILABLE = True
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    print(f"âš ï¸ TensorFlowä¸å¯ç”¨: {e}ï¼Œå°†è·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹")
    # åˆ›å»ºè™šæ‹Ÿå¯¹è±¡é¿å…ç±»å‹æ£€æŸ¥é”™è¯¯
    class DummyTF:
        @staticmethod
        def random():
            return type('obj', (object,), {'set_seed': lambda x: None})()
        
        @staticmethod
        def keras():
            return type('obj', (object,), {
                'Model': object,
                'optimizers': type('obj', (object,), {'Adam': object})()
            })()
    
    tf = DummyTF()  # type: ignore
    keras = DummyTF().keras()  # type: ignore

# å¯è§£é‡Šæ€§åˆ†æ
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("âš ï¸ SHAPä¸å¯ç”¨ï¼Œå°†è·³è¿‡å¯è§£é‡Šæ€§åˆ†æ")

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoostä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹")

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
if TENSORFLOW_AVAILABLE:
    tf.random.set_seed(42)

print("ç¢³ä»·æ ¼é¢„æµ‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
if TENSORFLOW_AVAILABLE:
    print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
if SHAP_AVAILABLE:
    print(f"SHAPç‰ˆæœ¬: {shap.__version__}")


class CarbonPricePredictionSystem:
    """ç¢³ä»·æ ¼é¢„æµ‹ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, config=None, output_dir=None):
        """åˆå§‹åŒ–ç³»ç»Ÿé…ç½®"""
        self.config = config or DEFAULT_CONFIG.copy()
        self.data = None
        self.processed_data = None
        self.models = {}
        self.predictions = {}
        self.shap_values = {}
        self.feature_names = []
        self.scalers = {}
        
        # è®¾ç½®è¾“å‡ºç›®å½•ç»“æ„
        self.base_dir = output_dir or "."
        self.program_name = FILE_NAME_FORMAT['program_name']
        self.run_timestamp = datetime.now().strftime(FILE_NAME_FORMAT['timestamp_format'])
        self.run_name = f"{self.program_name}_{self.run_timestamp}"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dirs = {
            key: os.path.join(self.base_dir, path)
            for key, path in OUTPUT_DIRS.items()
        }
        
        for dir_path in self.output_dirs.values():
            os.makedirs(dir_path, exist_ok=True)
        
    def _default_config(self):
        """
        é»˜è®¤é…ç½®
        
        ğŸ”§ é…ç½®å‚æ•°è¯´æ˜ï¼š
        =================
        
        target_column: ç›®æ ‡åˆ—åï¼ˆç¢³ä»·æ ¼åˆ—ï¼‰
        - é»˜è®¤: 'carbon_price'
        - å¦‚æœä½ çš„æ•°æ®åˆ—åä¸åŒï¼Œéœ€è¦ä¿®æ”¹æ­¤å‚æ•°
        - å¸¸è§åˆ—å: 'price', 'ç¢³ä»·æ ¼', 'carbon_price_eur', 'emission_price' ç­‰
        
        sequence_length: æ—¶é—´åºåˆ—é•¿åº¦
        - é»˜è®¤: 60 (å¤©)
        - ç”¨äºLSTMå’ŒTransformeræ¨¡å‹çš„è¾“å…¥åºåˆ—é•¿åº¦
        - å»ºè®®èŒƒå›´: 30-120ï¼Œå–å†³äºæ•°æ®é¢‘ç‡å’Œé¢„æµ‹æ—¶é—´è·¨åº¦
        
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        - é»˜è®¤: 0.2 (20%)
        - ç”¨äºæœ€ç»ˆæ¨¡å‹æ€§èƒ½è¯„ä¼°çš„æ•°æ®æ¯”ä¾‹
        
        validation_size: éªŒè¯é›†æ¯”ä¾‹
        - é»˜è®¤: 0.1 (10%)
        - ç”¨äºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„éªŒè¯å’Œè°ƒå‚
        
        lstm_config: LSTMæ¨¡å‹é…ç½®
        - units: éšè—å±‚å•å…ƒæ•° [72, 36]
        - dropout: éšæœºå¤±æ´»ç‡ 0.16
        - epochs: è®­ç»ƒè½®æ•° 140
        - batch_size: æ‰¹æ¬¡å¤§å° 8
        
        transformer_config: Transformeræ¨¡å‹é…ç½®
        - d_model: æ¨¡å‹ç»´åº¦ 16
        - num_heads: æ³¨æ„åŠ›å¤´æ•° 2
        - num_layers: ç¼–ç å™¨å±‚æ•° 2
        - dff: å‰é¦ˆç½‘ç»œç»´åº¦ 64
        - dropout: éšæœºå¤±æ´»ç‡ 0.6
        - epochs: è®­ç»ƒè½®æ•° 100
        - batch_size: æ‰¹æ¬¡å¤§å° 8
        
        ğŸ’¡ å¦‚ä½•è‡ªå®šä¹‰é…ç½®ï¼š
        --------------------
        custom_config = {
            'target_column': 'ä½ çš„åˆ—å',     # ä¿®æ”¹ç›®æ ‡åˆ—å
            'sequence_length': 90,           # å¢åŠ åºåˆ—é•¿åº¦
            'test_size': 0.15,              # è°ƒæ•´æµ‹è¯•é›†æ¯”ä¾‹
            'lstm_config': {
                'units': [128, 64, 32],      # æ›´å¤æ‚çš„ç½‘ç»œç»“æ„
                'epochs': 200,               # æ›´å¤šè®­ç»ƒè½®æ•°
                'batch_size': 16             # æ›´å°çš„æ‰¹æ¬¡å¤§å°
            }
        }
        system = CarbonPricePredictionSystem(config=custom_config)
        
        âš ï¸ æ³¨æ„ï¼šæ­¤æ–¹æ³•å·²åºŸå¼ƒï¼Œè¯·ç›´æ¥ä½¿ç”¨å…¨å±€ DEFAULT_CONFIG å˜é‡
        """
        # ç›´æ¥è¿”å›å…¨å±€DEFAULT_CONFIGçš„å‰¯æœ¬ï¼Œç¡®ä¿ä¸€è‡´æ€§
        return DEFAULT_CONFIG.copy()
    
    def load_data(self, file_path, sheet_name=None):
        try:
            print(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
            
            if file_path.endswith('.xlsx') or file_path.endswith('.xls'):
                # è¯»å–Excelæ–‡ä»¶ï¼Œå¤„ç†å¤šå·¥ä½œè¡¨æƒ…å†µ
                excel_data = pd.read_excel(file_path, sheet_name=sheet_name, index_col=0, parse_dates=True)
                
                # å¦‚æœsheet_name=Noneï¼Œä¼šè¿”å›å­—å…¸ï¼Œéœ€è¦é€‰æ‹©ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
                if isinstance(excel_data, dict):
                    # è·å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
                    first_sheet_name = list(excel_data.keys())[0]
                    self.data = excel_data[first_sheet_name]
                    print(f"æ£€æµ‹åˆ°å¤šä¸ªå·¥ä½œè¡¨ï¼Œè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨: {first_sheet_name}")
                else:
                    self.data = excel_data
            elif file_path.endswith('.csv'):
                self.data = pd.read_csv(file_path, index_col=0, parse_dates=True)
            elif file_path.endswith('.dta'):
                # è¯»å–Stataæ–‡ä»¶
                if PANDAS_STATA_SUPPORT:
                    # ä½¿ç”¨pandasè¯»å–Stataæ–‡ä»¶
                    stata_data = pd.read_stata(file_path)
                    # ç¡®ä¿æ•°æ®æ˜¯DataFrameç±»å‹
                    if isinstance(stata_data, pd.DataFrame):
                        self.data = stata_data
                    else:
                        self.data = pd.DataFrame(stata_data)
                    # è®¾ç½®ç¬¬ä¸€åˆ—ä¸ºç´¢å¼•ï¼ˆæ—¥æœŸï¼‰å¹¶è§£æä¸ºæ—¥æœŸç±»å‹
                    if not isinstance(self.data.index, pd.DatetimeIndex) and len(self.data.columns) > 0:
                        # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ—¥æœŸåˆ—
                        date_col = self.data.columns[0]
                        self.data[date_col] = pd.to_datetime(self.data[date_col], errors='coerce')
                        self.data.set_index(date_col, inplace=True)
                    print(f"æˆåŠŸè¯»å–Stataæ–‡ä»¶: {file_path}")
                else:
                    raise ValueError("å½“å‰pandasç‰ˆæœ¬ä¸æ”¯æŒè¯»å–Stataæ–‡ä»¶")
            else:
                raise ValueError("æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .xlsx, .xls, .csv, .dta")
            
            print(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {self.data.shape}")
            print(f"æ•°æ®åˆ—: {list(self.data.columns)}")
            print(f"æ•°æ®æ—¶é—´èŒƒå›´: {self.data.index[0]} åˆ° {self.data.index[-1]}")
            
            # æ£€æŸ¥å¿…è¦åˆ—
            if self.config['target_column'] not in self.data.columns:
                available_cols = list(self.data.columns)
                print(f"è­¦å‘Š: æœªæ‰¾åˆ°ç›®æ ‡åˆ— '{self.config['target_column']}'")
                print(f"å¯ç”¨åˆ—: {available_cols}")
                # å°è¯•çŒœæµ‹ç¢³ä»·æ ¼åˆ—
                possible_cols = [col for col in available_cols if any(keyword in col.lower() for keyword in ['price', 'carbon', 'ä»·æ ¼', 'ç¢³'])]
                if possible_cols:
                    self.config['target_column'] = possible_cols[0]
                    print(f"è‡ªåŠ¨é€‰æ‹©ç›®æ ‡åˆ—: {self.config['target_column']}")
                else:
                    raise ValueError(f"è¯·æŒ‡å®šæ­£ç¡®çš„ç¢³ä»·æ ¼åˆ—åï¼Œå¯ç”¨åˆ—: {available_cols}")
            
            return self.data
            
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def create_sample_data(self, start_date='2020-01-01', end_date='2023-12-31', save_path=None):
        """
        åˆ›å»ºç¤ºä¾‹ç¢³ä»·æ ¼æ•°æ®
        
        ğŸ“Š æ­¤å‡½æ•°ç”¨äºç”Ÿæˆæ¼”ç¤ºæ•°æ®ï¼Œå¦‚æœä½ æœ‰è‡ªå·±çš„æ•°æ®ï¼Œå¯ä»¥è·³è¿‡æ­¤æ­¥
        ================================================================
        
        å‚æ•°:
            start_date: å¼€å§‹æ—¥æœŸï¼ˆé»˜è®¤:'2020-01-01'ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆé»˜è®¤:'2023-12-31'ï¼‰
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
        ğŸ”§ ç”Ÿæˆçš„ç¤ºä¾‹æ•°æ®åŒ…å«ï¼š
        ---------------------------
        - carbon_price: ç¢³ä»·æ ¼ï¼ˆç›®æ ‡å˜é‡ï¼‰
        - gdp_growth: GDPå¢é•¿ç‡
        - industrial_production: å·¥ä¸šç”Ÿäº§æŒ‡æ•°
        - oil_price: çŸ³æ²¹ä»·æ ¼
        - gas_price: å¤©ç„¶æ°”ä»·æ ¼
        - electricity_demand: ç”µåŠ›éœ€æ±‚
        - temperature: æ¸©åº¦
        - policy_impact: æ”¿ç­–å½±å“æŒ‡æ•°
        - tech_innovation: æŠ€æœ¯åˆ›æ–°æŒ‡æ•°
        - emissions: ç¢³æ’æ”¾é‡
        - ä»¥åŠå„ç§æŠ€æœ¯æŒ‡æ ‡ï¼ˆæ»åã€ç§»åŠ¨å¹³å‡ã€æ³¢åŠ¨ç‡ç­‰ï¼‰
        
        ğŸ’¡ å¦‚æœä½ æœ‰è‡ªå·±çš„æ•°æ®ï¼š
        -------------------------
        1. è·³è¿‡æ­¤å‡½æ•°ï¼Œç›´æ¥ä½¿ç”¨ load_data() åŠ è½½ä½ çš„æ•°æ®
        2. ç¡®ä¿ä½ çš„æ•°æ®åŒ…å«ç±»ä¼¼çš„åˆ—ç»“æ„
        3. æˆ–è€…å‚è€ƒæ­¤å‡½æ•°ç”Ÿæˆçš„æ•°æ®æ ¼å¼æ¥å‡†å¤‡ä½ çš„æ•°æ®
        """
        print("åˆ›å»ºç¤ºä¾‹ç¢³ä»·æ ¼æ•°æ®...")
        
        dates = pd.date_range(start=start_date, end=end_date, freq='D')
        n_days = len(dates)
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        np.random.seed(42)
        
        # æ¨¡æ‹Ÿç¢³ä»·æ ¼æ•°æ®ï¼ˆå‚è€ƒæ¬§ç›ŸETSç­‰ç¢³å¸‚åœºï¼‰
        base_price = 50  # åŸºç¡€ä»·æ ¼
        
        # è¶‹åŠ¿åˆ†é‡ï¼ˆé•¿æœŸä¸Šå‡è¶‹åŠ¿ï¼‰
        trend = np.linspace(0, 30, n_days)
        
        # å­£èŠ‚æ€§åˆ†é‡
        seasonal = 5 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) + \
                  2 * np.sin(2 * np.pi * np.arange(n_days) / (365.25/12))
        
        # éšæœºæ³¢åŠ¨
        noise = np.random.normal(0, 3, n_days)
        
        # ä»·æ ¼å†²å‡»äº‹ä»¶
        shock_days = np.random.choice(n_days, size=10, replace=False)
        shocks = np.zeros(n_days)
        for day in shock_days:
            shocks[day:day+5] = np.random.normal(0, 8)
        
        carbon_price = base_price + trend + seasonal + noise + shocks
        carbon_price = np.maximum(carbon_price, 5)  # ç¡®ä¿ä»·æ ¼ä¸ä¸ºè´Ÿ
        
        # ç›¸å…³å½±å“å› å­
        data = pd.DataFrame(index=dates)
        data['carbon_price'] = carbon_price
        
        # GDPå¢é•¿ç‡å½±å“
        gdp_growth = 2 + 0.5 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) + np.random.normal(0, 1, n_days)
        data['gdp_growth'] = gdp_growth
        
        # å·¥ä¸šç”Ÿäº§æŒ‡æ•°
        industrial_production = 100 + np.cumsum(np.random.normal(0.01, 0.5, n_days))
        data['industrial_production'] = industrial_production
        
        # èƒ½æºä»·æ ¼ï¼ˆçŸ³æ²¹ã€å¤©ç„¶æ°”ï¼‰
        oil_price = 60 + 20 * np.sin(2 * np.pi * np.arange(n_days) / 180) + np.random.normal(0, 5, n_days)
        data['oil_price'] = np.maximum(oil_price, 20)
        
        gas_price = 3 + 1.5 * np.sin(2 * np.pi * np.arange(n_days) / 120) + np.random.normal(0, 0.8, n_days)
        data['gas_price'] = np.maximum(gas_price, 1)
        
        # ç”µåŠ›éœ€æ±‚
        electricity_demand = 1000 + 200 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) + \
                           50 * np.sin(2 * np.pi * np.arange(n_days) / 7) + np.random.normal(0, 30, n_days)
        data['electricity_demand'] = electricity_demand
        
        # æ¸©åº¦ï¼ˆå½±å“èƒ½æºéœ€æ±‚ï¼‰
        temperature = 15 + 10 * np.sin(2 * np.pi * (np.arange(n_days) - 80) / 365.25) + np.random.normal(0, 3, n_days)
        data['temperature'] = temperature
        
        # æ”¿ç­–æŒ‡æ•°ï¼ˆæ¨¡æ‹Ÿæ”¿ç­–å½±å“ï¼‰
        policy_impact = np.cumsum(np.random.choice([0, 0, 0, 1, -1], n_days, p=[0.7, 0.1, 0.1, 0.05, 0.05]))
        data['policy_impact'] = policy_impact
        
        # æŠ€æœ¯åˆ›æ–°æŒ‡æ•°
        tech_innovation = np.cumsum(np.random.exponential(0.1, n_days))
        data['tech_innovation'] = tech_innovation
        
        # ç¢³æ’æ”¾é‡
        emissions = 1000 - 0.1 * tech_innovation + np.random.normal(0, 50, n_days)
        data['emissions'] = np.maximum(emissions, 500)
        
        # æ·»åŠ æ»åå˜é‡å’ŒæŠ€æœ¯æŒ‡æ ‡
        data['carbon_price_lag1'] = data['carbon_price'].shift(1)
        data['carbon_price_lag7'] = data['carbon_price'].shift(7)
        data['carbon_price_ma7'] = data['carbon_price'].rolling(7).mean()
        data['carbon_price_ma30'] = data['carbon_price'].rolling(30).mean()
        data['price_volatility'] = data['carbon_price'].rolling(14).std()
        
        # ç§»é™¤NaNå€¼
        data = data.dropna()
        
        self.data = data
        print(f"ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆï¼Œå½¢çŠ¶: {data.shape}")
        
        if save_path:
            save_file = os.path.join(self.output_dirs['excel'], f"{self.run_name}_sample_data.xlsx")
            data.to_excel(save_file)
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {save_file}")
        
        return data
    
    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹"""
        print("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        if self.data is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ•°æ®")
        
        df = self.data.copy()
        
        # åŸºç¡€ç‰¹å¾å·¥ç¨‹
        target_col = self.config['target_column']
        
        # é¦–å…ˆæ£€æŸ¥å¹¶å¤„ç†åŸå§‹æ•°æ®ä¸­çš„NaNå€¼
        print(f"åŸå§‹æ•°æ®NaNç»Ÿè®¡: {df.isnull().sum().sum()} ä¸ª")
        
        # è¯†åˆ«å¹¶ç§»é™¤å…¨ä¸ºNaNçš„åˆ—
        null_cols = df.columns[df.isnull().all()].tolist()
        if null_cols:
            print(f"âš ï¸  å‘ç°å…¨ä¸ºNaNçš„åˆ—: {null_cols}")
            print(f"   è¿™äº›åˆ—å°†è¢«ç§»é™¤ï¼Œå› ä¸ºæ— æ³•é€šè¿‡æ’å€¼æ¢å¤")
            df = df.drop(columns=null_cols)
        
        # è¯†åˆ«NaNæ¯”ä¾‹è¿‡é«˜çš„åˆ—ï¼ˆè¶…è¿‡80%ï¼‰
        high_nan_cols = []
        for col in df.columns:
            nan_ratio = df[col].isnull().sum() / len(df)
            if nan_ratio > 0.8:
                high_nan_cols.append((col, nan_ratio))
        
        if high_nan_cols:
            print(f"âš ï¸  å‘ç°NaNæ¯”ä¾‹è¿‡é«˜(>80%)çš„åˆ—:")
            for col, ratio in high_nan_cols:
                print(f"   {col}: {ratio*100:.1f}% NaN")
            print(f"   å»ºè®®ç§»é™¤è¿™äº›åˆ—ä»¥æé«˜æ•°æ®è´¨é‡")
            # è‡ªåŠ¨ç§»é™¤NaNæ¯”ä¾‹è¶…è¿‡80%çš„åˆ—
            cols_to_drop = [col for col, _ in high_nan_cols]
            df = df.drop(columns=cols_to_drop)
            print(f"   å·²ç§»é™¤ {len(cols_to_drop)} ä¸ªä½è´¨é‡åˆ—")
        
        # ä½¿ç”¨å¤šç§æ’å€¼æ–¹æ³•å¡«å……åŸå§‹æ•°æ®ä¸­çš„NaNï¼ˆæ›´ç¨³å¥çš„æ–¹æ³•ï¼‰
        for col in df.columns:
            if df[col].isnull().any():
                # 1. é¦–å…ˆå°è¯•çº¿æ€§æ’å€¼ï¼ˆåŒå‘ï¼‰
                df[col] = df[col].interpolate(method='linear', limit_direction='both')
                
                # 2. å¯¹äºé¦–å°¾çš„NaNå€¼ï¼Œä½¿ç”¨å¤šé¡¹å¼æ’å€¼
                if df[col].isnull().any():
                    df[col] = df[col].interpolate(method='polynomial', order=2, limit_direction='both')
                
                # 3. ä½¿ç”¨å‰å‘å¡«å……å¤„ç†å‰©ä½™çš„NaN
                if df[col].isnull().any():
                    df[col] = df[col].bfill()
                
                # 4. ä½¿ç”¨åå‘å¡«å……å¤„ç†å‰©ä½™çš„NaN
                if df[col].isnull().any():
                    df[col] = df[col].ffill()
                
                # 5. å¦‚æœè¿˜æœ‰NaNï¼ˆæç«¯æƒ…å†µï¼‰ï¼Œç”¨åˆ—å‡å€¼å¡«å……
                if df[col].isnull().any():
                    df[col] = df[col].fillna(df[col].mean())
                
                # 6. æœ€åçš„ä¿é™©æªæ–½ï¼šç”¨ä¸­ä½æ•°å¡«å……
                if df[col].isnull().any():
                    df[col] = df[col].fillna(df[col].median())
        
        print(f"åŸå§‹æ•°æ®NaNå¤„ç†å: {df.isnull().sum().sum()} ä¸ª")
        print(f"ä¿ç•™çš„åˆ—æ•°: {len(df.columns)}")
        
        # ä»·æ ¼å˜åŒ–ç‰¹å¾
        df['price_return'] = df[target_col].pct_change()
        df['price_diff'] = df[target_col].diff()
        
        # ç§»åŠ¨å¹³å‡ç‰¹å¾
        for window in [5, 10, 20, 30]:
            df[f'ma_{window}'] = df[target_col].rolling(window, min_periods=1).mean()
            df[f'ma_{window}_ratio'] = df[target_col] / df[f'ma_{window}']
        
        # æ³¢åŠ¨ç‡ç‰¹å¾
        for window in [7, 14, 30]:
            df[f'volatility_{window}'] = df['price_return'].rolling(window, min_periods=1).std()
        
        # æŠ€æœ¯æŒ‡æ ‡
        # RSI
        delta = df[target_col].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=1).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=1).mean()
        rs = gain / (loss + 1e-10)  # é¿å…é™¤ä»¥0
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # å¸ƒæ—å¸¦
        bb_window = 20
        df['bb_middle'] = df[target_col].rolling(bb_window, min_periods=1).mean()
        bb_std = df[target_col].rolling(bb_window, min_periods=1).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_width'] = df['bb_upper'] - df['bb_lower']
        # é¿å…é™¤ä»¥0
        df['bb_position'] = (df[target_col] - df['bb_lower']) / (df['bb_width'] + 1e-10)
        
        # ä»·æ ¼åŠ¨é‡
        for period in [5, 10, 20]:
            df[f'momentum_{period}'] = df[target_col].diff(period)
        
        # æ»åç‰¹å¾
        for lag in [1, 2, 3, 5, 10]:
            df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)
        
        # ç§»é™¤æ— æ•ˆå€¼
        df = df.replace([np.inf, -np.inf], np.nan)
        
        print(f"ç‰¹å¾å·¥ç¨‹åæ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"ç‰¹å¾å·¥ç¨‹åNaNç»Ÿè®¡: {df.isnull().sum().sum()} ä¸ª")
        
        # å†æ¬¡ä½¿ç”¨å¤šå±‚æ¬¡æ’å€¼æ³•å¤„ç†è¡ç”Ÿç‰¹å¾äº§ç”Ÿçš„NaN
        for col in df.columns:
            if df[col].isnull().any():
                # 1. çº¿æ€§æ’å€¼ï¼ˆåŒå‘ï¼‰
                df[col] = df[col].interpolate(method='linear', limit_direction='both')
                
                # 2. æ—¶é—´åºåˆ—æ’å€¼ï¼ˆé’ˆå¯¹æ—¶é—´ç›¸å…³çš„ç‰¹å¾ï¼‰
                if df[col].isnull().any():
                    try:
                        df[col] = df[col].interpolate(method='time')
                    except:
                        pass  # å¦‚æœæ—¶é—´æ’å€¼å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å…¶ä»–æ–¹æ³•
                
                # 3. æ ·æ¡æ’å€¼ï¼ˆæ›´å¹³æ»‘ï¼‰
                if df[col].isnull().any():
                    try:
                        df[col] = df[col].interpolate(method='spline', order=3, limit_direction='both')
                    except:
                        pass  # å¦‚æœæ ·æ¡æ’å€¼å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å…¶ä»–æ–¹æ³•
                
                # 4. å‰å‘å¡«å……
                if df[col].isnull().any():
                    df[col] = df[col].bfill()
                
                # 5. åå‘å¡«å……
                if df[col].isnull().any():
                    df[col] = df[col].ffill()
                
                # 6. å‡å€¼å¡«å……
                if df[col].isnull().any():
                    df[col] = df[col].fillna(df[col].mean())
                
                # 7. ä¸­ä½æ•°å¡«å……ï¼ˆæ›´ç¨³å¥ï¼‰
                if df[col].isnull().any():
                    df[col] = df[col].fillna(df[col].median())
        
        # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ²¡æœ‰NaNå€¼
        remaining_nan = df.isnull().sum().sum()
        if remaining_nan > 0:
            print(f"âš ï¸  è­¦å‘Š: ç»è¿‡å¤šå±‚æ’å€¼åä»æœ‰ {remaining_nan} ä¸ªNaNå€¼")
            # æ˜¾ç¤ºå“ªäº›åˆ—è¿˜æœ‰NaN
            nan_cols = df.columns[df.isnull().any()].tolist()
            print(f"   åŒ…å«NaNçš„åˆ—: {nan_cols[:10]}{'...' if len(nan_cols) > 10 else ''}")
            
            # æœ€åä½¿ç”¨0å¡«å……ï¼ˆä½œä¸ºæœ€åçš„ä¿é™©æªæ–½ï¼‰
            print(f"   ä½¿ç”¨0å¡«å……ä½œä¸ºæœ€åçš„å¤„ç†æªæ–½")
            df = df.fillna(0)
        else:
            print("âœ… æ•°æ®é¢„å¤„ç†æˆåŠŸï¼šæ‰€æœ‰NaNå€¼å·²é€šè¿‡æ’å€¼æ–¹æ³•å¤„ç†")
        
        print(f"æœ€ç»ˆæ•°æ®NaNæ£€æŸ¥: {df.isnull().sum().sum()} ä¸ª")
        
        # æ£€æŸ¥æ•°æ®é‡
        if len(df) == 0:
            raise ValueError("é¢„å¤„ç†åæ•°æ®ä¸ºç©º")
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        exclude_cols = [target_col, 'price_return', 'price_diff']
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        self.feature_names = feature_cols
        self.processed_data = df
        
        print(f"é¢„å¤„ç†å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        # åªæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾ä»¥é¿å…è¾“å‡ºè¿‡é•¿
        display_features = feature_cols[:10] + (['...'] if len(feature_cols) > 10 else [])
        print(f"ç‰¹å¾åˆ—è¡¨: {display_features}")
        
        return df
    
    def prepare_sequences(self, data, target_col, feature_cols, seq_length):
        """å‡†å¤‡åºåˆ—æ•°æ®ç”¨äºLSTMå’ŒTransformer"""
        sequences = []
        targets = []
        
        for i in range(seq_length, len(data)):
            seq = data[feature_cols].iloc[i-seq_length:i].values
            target = data[target_col].iloc[i]
            sequences.append(seq)
            targets.append(target)
        
        return np.array(sequences), np.array(targets)
    
    def split_data(self):
        """åˆ†å‰²è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®"""
        if self.processed_data is None:
            raise ValueError("è¯·å…ˆè¿›è¡Œæ•°æ®é¢„å¤„ç†")
        
        target_col = self.config['target_column']
        test_size = self.config['test_size']
        val_size = self.config['validation_size']
        
        # æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆä¿æŒæ—¶é—´é¡ºåºï¼‰
        n = len(self.processed_data)
        train_end = int(n * (1 - test_size - val_size))
        val_end = int(n * (1 - test_size))
        
        train_data = self.processed_data.iloc[:train_end]
        val_data = self.processed_data.iloc[train_end:val_end]
        test_data = self.processed_data.iloc[val_end:]
        
        print(f"æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
        
        return train_data, val_data, test_data
    
    def build_lstm_model(self):
        """æ„å»ºLSTMæ¨¡å‹"""
        print("æ„å»ºLSTMæ¨¡å‹...")
        
        config = self.config['lstm_config']
        seq_length = self.config['sequence_length']
        n_features = len(self.feature_names)
        
        model = Sequential()
        
        # ç¬¬ä¸€å±‚LSTM
        model.add(LSTM(
            units=config['units'][0],
            return_sequences=True,
            input_shape=(seq_length, n_features)
        ))
        model.add(Dropout(config['dropout']))
        
        # ç¬¬äºŒå±‚LSTM
        if len(config['units']) > 1:
            model.add(LSTM(
                units=config['units'][1],
                return_sequences=False
            ))
            model.add(Dropout(config['dropout']))
        
        # è¾“å‡ºå±‚
        model.add(Dense(1))
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        print(f"LSTMæ¨¡å‹æ¶æ„:")
        model.summary()
        
        return model
    
    def build_transformer_model(self):
        """æ„å»ºTransformeræ¨¡å‹"""
        print("æ„å»ºTransformeræ¨¡å‹...")
        
        config = self.config['transformer_config']
        seq_length = self.config['sequence_length']
        n_features = len(self.feature_names)
        
        # è¾“å…¥å±‚
        inputs = layers.Input(shape=(seq_length, n_features))
        
        # æŠ•å½±åˆ°d_modelç»´åº¦
        x = layers.Dense(config['d_model'])(inputs)
        
        # ä½ç½®ç¼–ç 
        x = self._add_positional_encoding(x, seq_length, config['d_model'])
        
        # Transformerç¼–ç å™¨å±‚
        for _ in range(config['num_layers']):
            x = self._transformer_encoder(
                x, 
                config['d_model'], 
                config['num_heads'], 
                config['dff'],
                config['dropout']
            )
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = layers.GlobalAveragePooling1D()(x)
        
        # è¾“å‡ºå±‚
        outputs = layers.Dense(1)(x)
        
        model = keras.Model(inputs=inputs, outputs=outputs)
        
        # è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦
        learning_rate = self._create_lr_schedule(config['d_model'])
        optimizer = keras.optimizers.Adam(
            learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9
        )
        
        model.compile(
            optimizer=optimizer,
            loss='mse',
            metrics=['mae']
        )
        
        print(f"Transformeræ¨¡å‹æ¶æ„:")
        model.summary()
        
        return model
    
    def _add_positional_encoding(self, x, seq_len, d_model):
        """æ·»åŠ ä½ç½®ç¼–ç """
        def get_angles(pos, i, d_model):
            angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))
            return pos * angle_rates
        
        angle_rads = get_angles(
            np.arange(seq_len)[:, np.newaxis],
            np.arange(d_model)[np.newaxis, :],
            d_model
        )
        
        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
        
        pos_encoding = angle_rads[np.newaxis, ...]
        pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)
        
        return x + pos_encoding
    
    def _transformer_encoder(self, x, d_model, num_heads, dff, dropout_rate):
        """Transformerç¼–ç å™¨å±‚"""
        # å¤šå¤´æ³¨æ„åŠ›
        attn_output = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=d_model
        )(x, x)
        attn_output = layers.Dropout(dropout_rate)(attn_output)
        out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)
        
        # å‰é¦ˆç½‘ç»œ
        ffn_output = layers.Dense(dff, activation='relu')(out1)
        ffn_output = layers.Dense(d_model)(ffn_output)
        ffn_output = layers.Dropout(dropout_rate)(ffn_output)
        out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)
        
        return out2
    
    def _create_lr_schedule(self, d_model, warmup_steps=4000):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦"""
        class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):
            def __init__(self, d_model, warmup_steps=4000):
                super(CustomSchedule, self).__init__()
                self.d_model = d_model
                self.d_model = tf.cast(self.d_model, tf.float32)
                self.warmup_steps = warmup_steps
            
            def __call__(self, step):
                step = tf.cast(step, tf.float32)
                arg1 = tf.math.rsqrt(step)
                arg2 = step * (self.warmup_steps ** -1.5)
                return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
        
        return CustomSchedule(d_model, warmup_steps)
    
    def build_ml_models(self):
        """æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹ç”¨äºSHAPåˆ†æ"""
        print("æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹...")
        
        models = {
            'RandomForest': RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            ),
            'GradientBoosting': GradientBoostingRegressor(
                n_estimators=100,
                max_depth=6,
                random_state=42
            )
        }
        
        # åªåœ¨XGBoostå¯ç”¨æ—¶æ·»åŠ 
        if XGBOOST_AVAILABLE:
            models['XGBoost'] = xgb.XGBRegressor(
                n_estimators=100,
                max_depth=6,
                random_state=42,
                n_jobs=-1
            )
        
        return models
    
    def train_models(self):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("="*60)
        print("å¼€å§‹è®­ç»ƒæ¨¡å‹")
        print("="*60)
        
        if self.processed_data is None:
            raise ValueError("è¯·å…ˆè¿›è¡Œæ•°æ®é¢„å¤„ç†")
        
        # åˆ†å‰²æ•°æ®
        train_data, val_data, test_data = self.split_data()
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿ
        if len(train_data) == 0:
            raise ValueError("è®­ç»ƒæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æºæˆ–å‡å°‘åºåˆ—é•¿åº¦")
        
        target_col = self.config['target_column']
        seq_length = self.config['sequence_length']
        
        # æ£€æŸ¥åºåˆ—é•¿åº¦æ˜¯å¦åˆç†
        if seq_length >= len(train_data):
            raise ValueError(f"åºåˆ—é•¿åº¦({seq_length})å¿…é¡»å°äºè®­ç»ƒæ•°æ®é•¿åº¦({len(train_data)})")
        
        # å‡†å¤‡åºåˆ—æ•°æ®
        X_seq_train, y_seq_train = self.prepare_sequences(
            train_data, target_col, self.feature_names, seq_length
        )
        X_seq_val, y_seq_val = self.prepare_sequences(
            val_data, target_col, self.feature_names, seq_length
        )
        X_seq_test, y_seq_test = self.prepare_sequences(
            test_data, target_col, self.feature_names, seq_length
        )
        
        # æ£€æŸ¥åºåˆ—æ•°æ®æ˜¯å¦ä¸ºç©º
        if len(X_seq_train) == 0:
            raise ValueError(f"è®­ç»ƒåºåˆ—æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ–å‡å°åºåˆ—é•¿åº¦ã€‚å½“å‰åºåˆ—é•¿åº¦: {seq_length}, è®­ç»ƒæ•°æ®é•¿åº¦: {len(train_data)}")
        
        # å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®
        X_ml_train = train_data[self.feature_names].values
        y_ml_train = train_data[target_col].values
        X_ml_test = test_data[self.feature_names].values
        y_ml_test = test_data[target_col].values
        
        # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼ï¼ˆåº”è¯¥å·²ç»åœ¨preprocess_dataä¸­å¤„ç†å®Œæ¯•ï¼‰
        print(f"\næ•°æ®å®Œæ•´æ€§æ£€æŸ¥:")
        print(f"X_seq_train NaNæ•°é‡: {np.isnan(X_seq_train).sum()}")
        print(f"y_seq_train NaNæ•°é‡: {np.isnan(y_seq_train).sum()}")
        print(f"X_seq_test NaNæ•°é‡: {np.isnan(X_seq_test).sum()}")
        print(f"y_seq_test NaNæ•°é‡: {np.isnan(y_seq_test).sum()}")
        
        # å¦‚æœè¿˜æœ‰NaNå€¼ï¼ˆç†è®ºä¸Šä¸åº”è¯¥æœ‰ï¼‰ï¼Œç›´æ¥æŠ¥é”™è€Œä¸æ˜¯é™é»˜å¡«å……
        if np.isnan(X_seq_train).any() or np.isnan(y_seq_train).any():
            raise ValueError("è®­ç»ƒæ•°æ®ä¸­ä»æœ‰NaNå€¼ï¼Œè¯·æ£€æŸ¥preprocess_dataæ­¥éª¤")
        
        if np.isnan(X_seq_test).any() or np.isnan(y_seq_test).any():
            raise ValueError("æµ‹è¯•æ•°æ®ä¸­ä»æœ‰NaNå€¼ï¼Œè¯·æ£€æŸ¥preprocess_dataæ­¥éª¤")
        
        if np.isnan(X_ml_train).any() or np.isnan(y_ml_train).any():
            raise ValueError("MLè®­ç»ƒæ•°æ®ä¸­ä»æœ‰NaNå€¼ï¼Œè¯·æ£€æŸ¥preprocess_dataæ­¥éª¤")
        
        if np.isnan(X_ml_test).any() or np.isnan(y_ml_test).any():
            raise ValueError("MLæµ‹è¯•æ•°æ®ä¸­ä»æœ‰NaNå€¼ï¼Œè¯·æ£€æŸ¥preprocess_dataæ­¥éª¤")
        
        # å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œç‰¹å¾æ ‡å‡†åŒ–ï¼ˆè¿™å¯¹LSTMå’ŒTransformerå¾ˆé‡è¦ï¼‰
        print("\nå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œç‰¹å¾æ ‡å‡†åŒ–...")
        
        # åˆ›å»ºç‰¹å¾ç¼©æ”¾å™¨ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼‰
        self.scalers['X_scaler'] = MinMaxScaler(feature_range=(0, 1))
        self.scalers['y_scaler'] = MinMaxScaler(feature_range=(0, 1))
        
        # å¤„ç†åºåˆ—æ•°æ®çš„æ ‡å‡†åŒ–ï¼ˆ3Dæ•°ç»„ï¼‰
        # å°†3Dæ•°ç»„å±•å¼€ä¸º2Dè¿›è¡Œæ ‡å‡†åŒ–ï¼Œç„¶åå†æ¢å¤å½¢çŠ¶
        original_train_shape = X_seq_train.shape
        original_val_shape = X_seq_val.shape
        original_test_shape = X_seq_test.shape
        
        # è®­ç»ƒé›†ï¼šæ‹Ÿåˆå¹¶è½¬æ¢
        X_seq_train_2d = X_seq_train.reshape(-1, original_train_shape[-1])
        X_seq_train_2d_scaled = self.scalers['X_scaler'].fit_transform(X_seq_train_2d)
        X_seq_train_scaled = X_seq_train_2d_scaled.reshape(original_train_shape)
        
        # éªŒè¯é›†ï¼šä»…è½¬æ¢
        X_seq_val_2d = X_seq_val.reshape(-1, original_val_shape[-1])
        X_seq_val_2d_scaled = self.scalers['X_scaler'].transform(X_seq_val_2d)
        X_seq_val_scaled = X_seq_val_2d_scaled.reshape(original_val_shape)
        
        # æµ‹è¯•é›†ï¼šä»…è½¬æ¢
        X_seq_test_2d = X_seq_test.reshape(-1, original_test_shape[-1])
        X_seq_test_2d_scaled = self.scalers['X_scaler'].transform(X_seq_test_2d)
        X_seq_test_scaled = X_seq_test_2d_scaled.reshape(original_test_shape)
        
        # ç›®æ ‡å˜é‡æ ‡å‡†åŒ–
        y_seq_train_scaled = self.scalers['y_scaler'].fit_transform(
            y_seq_train.reshape(-1, 1)
        ).flatten()
        y_seq_val_scaled = self.scalers['y_scaler'].transform(
            y_seq_val.reshape(-1, 1)
        ).flatten()
        y_seq_test_scaled = self.scalers['y_scaler'].transform(
            y_seq_test.reshape(-1, 1)
        ).flatten()
        
        print(f"ç‰¹å¾ç¼©æ”¾èŒƒå›´: [{X_seq_train_scaled.min():.4f}, {X_seq_train_scaled.max():.4f}]")
        print(f"ç›®æ ‡ç¼©æ”¾èŒƒå›´: [{y_seq_train_scaled.min():.4f}, {y_seq_train_scaled.max():.4f}]")
        
        # MLæ•°æ®ä¸è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆæ ¹æ®é¡¹ç›®é…ç½®ï¼‰
        X_ml_train_scaled = X_ml_train
        X_ml_test_scaled = X_ml_test
        
        # è®­ç»ƒLSTMæ¨¡å‹
        print("\nè®­ç»ƒLSTMæ¨¡å‹...")
        lstm_model = self.build_lstm_model()
        
        # éªŒè¯è¾“å…¥æ•°æ®çš„æœ‰æ•ˆæ€§
        print(f"LSTMè¾“å…¥æ•°æ®æ£€æŸ¥:")
        print(f"  X_train shape: {X_seq_train_scaled.shape}, range: [{X_seq_train_scaled.min():.4f}, {X_seq_train_scaled.max():.4f}]")
        print(f"  y_train shape: {y_seq_train_scaled.shape}, range: [{y_seq_train_scaled.min():.4f}, {y_seq_train_scaled.max():.4f}]")
        
        lstm_history = lstm_model.fit(
            X_seq_train_scaled, y_seq_train_scaled,
            validation_data=(X_seq_val_scaled, y_seq_val_scaled),
            epochs=self.config['lstm_config']['epochs'],
            batch_size=self.config['lstm_config']['batch_size'],
            verbose=1,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(
                    patience=10, restore_best_weights=True, monitor='val_loss'
                ),
                tf.keras.callbacks.ReduceLROnPlateau(
                    patience=5, factor=0.5, monitor='val_loss'
                )
            ]
        )
        
        # è®­ç»ƒTransformeræ¨¡å‹
        print("\nè®­ç»ƒTransformeræ¨¡å‹...")
        transformer_model = self.build_transformer_model()
        
        # éªŒè¯è¾“å…¥æ•°æ®çš„æœ‰æ•ˆæ€§
        print(f"Transformerè¾“å…¥æ•°æ®æ£€æŸ¥:")
        print(f"  X_train shape: {X_seq_train_scaled.shape}, range: [{X_seq_train_scaled.min():.4f}, {X_seq_train_scaled.max():.4f}]")
        print(f"  y_train shape: {y_seq_train_scaled.shape}, range: [{y_seq_train_scaled.min():.4f}, {y_seq_train_scaled.max():.4f}]")
        
        transformer_history = transformer_model.fit(
            X_seq_train_scaled, y_seq_train_scaled,
            validation_data=(X_seq_val_scaled, y_seq_val_scaled),
            epochs=self.config['transformer_config']['epochs'],
            batch_size=self.config['transformer_config'].get('batch_size', 8),
            verbose=1,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(
                    patience=10, restore_best_weights=True, monitor='val_loss'
                )
            ]
        )
        
        # è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
        print("\nè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")
        ml_models = self.build_ml_models()
        
        # ä¸è¿›è¡Œæ ‡å‡†åŒ–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
        for name, model in ml_models.items():
            print(f"è®­ç»ƒ {name}...")
            model.fit(X_ml_train_scaled, y_ml_train)
        
        # ä¿å­˜æ¨¡å‹å’Œæ•°æ®
        self.models = {
            'lstm': lstm_model,
            'transformer': transformer_model,
            **ml_models
        }
        
        self.train_data = {
            'X_seq_train': X_seq_train_scaled,
            'y_seq_train': y_seq_train_scaled,
            'X_seq_test': X_seq_test_scaled,
            'y_seq_test': y_seq_test,
            'X_ml_train': X_ml_train_scaled,
            'y_ml_train': y_ml_train,
            'X_ml_test': X_ml_test_scaled,
            'y_ml_test': y_ml_test
        }
        
        self.train_history = {
            'lstm': lstm_history,
            'transformer': transformer_history
        }
        
        print("\næ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        return self.models
    
    def evaluate_models(self):
        """è¯„ä¼°æ‰€æœ‰æ¨¡å‹æ€§èƒ½"""
        print("="*60)
        print("æ¨¡å‹æ€§èƒ½è¯„ä¼°")
        print("="*60)
        
        if not self.models:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        
        results = {}
        
        # æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹
        for model_name in ['lstm', 'transformer']:
            if model_name in self.models:
                print(f"\nè¯„ä¼° {model_name.upper()} æ¨¡å‹...")
                model = self.models[model_name]
                
                # é¢„æµ‹
                y_pred_scaled = model.predict(
                    self.train_data['X_seq_test'], verbose=0
                )
                
                # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
                y_pred = self.scalers['y_scaler'].inverse_transform(
                    y_pred_scaled.reshape(-1, 1)
                ).flatten()
                
                # åæ ‡å‡†åŒ–çœŸå®å€¼ï¼ˆå¦‚æœä¹‹å‰è¿›è¡Œäº†æ ‡å‡†åŒ–ï¼‰
                if hasattr(self.scalers, 'y_scaler'):
                    # y_seq_testå·²ç»æ˜¯æ ‡å‡†åŒ–çš„ï¼Œéœ€è¦åæ ‡å‡†åŒ–
                    y_true_scaled = self.train_data['y_seq_test']
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ ‡å‡†åŒ–
                    if y_true_scaled.max() <= 1.0 and y_true_scaled.min() >= 0.0:
                        y_true = self.scalers['y_scaler'].inverse_transform(
                            y_true_scaled.reshape(-1, 1)
                        ).flatten()
                    else:
                        y_true = y_true_scaled
                else:
                    y_true = self.train_data['y_seq_test']
                
                # è®¡ç®—æŒ‡æ ‡
                mse = mean_squared_error(y_true, y_pred)
                mae = mean_absolute_error(y_true, y_pred)
                rmse = np.sqrt(mse)
                r2 = r2_score(y_true, y_pred)
                mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
                
                # æ–¹å‘å‡†ç¡®ç‡
                direction_accuracy = np.mean(
                    np.sign(y_pred[1:] - y_pred[:-1]) == 
                    np.sign(y_true[1:] - y_true[:-1])
                ) * 100
                
                results[model_name] = {
                    'MSE': mse,
                    'MAE': mae,
                    'RMSE': rmse,
                    'RÂ²': r2,
                    'MAPE': mape,
                    'Direction_Accuracy': direction_accuracy,
                    'predictions': y_pred,
                    'actual': y_true
                }
                
                print(f"MSE: {mse:.4f}")
                print(f"MAE: {mae:.4f}")
                print(f"RMSE: {rmse:.4f}")
                print(f"RÂ²: {r2:.4f}")
                print(f"MAPE: {mape:.2f}%")
                print(f"æ–¹å‘å‡†ç¡®ç‡: {direction_accuracy:.2f}%")
        
        # æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹
        for model_name in ['RandomForest', 'GradientBoosting', 'XGBoost']:
            if model_name in self.models:
                print(f"\nè¯„ä¼° {model_name} æ¨¡å‹...")
                model = self.models[model_name]
                
                # é¢„æµ‹
                y_pred = model.predict(self.train_data['X_ml_test'])
                y_true = self.train_data['y_ml_test']
                
                # è®¡ç®—æŒ‡æ ‡
                mse = mean_squared_error(y_true, y_pred)
                mae = mean_absolute_error(y_true, y_pred)
                rmse = np.sqrt(mse)
                r2 = r2_score(y_true, y_pred)
                mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
                
                # æ–¹å‘å‡†ç¡®ç‡
                direction_accuracy = np.mean(
                    np.sign(y_pred[1:] - y_pred[:-1]) == 
                    np.sign(y_true[1:] - y_true[:-1])
                ) * 100
                
                results[model_name] = {
                    'MSE': mse,
                    'MAE': mae,
                    'RMSE': rmse,
                    'RÂ²': r2,
                    'MAPE': mape,
                    'Direction_Accuracy': direction_accuracy,
                    'predictions': y_pred,
                    'actual': y_true
                }
                
                print(f"MSE: {mse:.4f}")
                print(f"MAE: {mae:.4f}")
                print(f"RMSE: {rmse:.4f}")
                print(f"RÂ²: {r2:.4f}")
                print(f"MAPE: {mape:.2f}%")
                print(f"æ–¹å‘å‡†ç¡®ç‡: {direction_accuracy:.2f}%")
        
        self.predictions = results
        
        # åˆ›å»ºæ€§èƒ½å¯¹æ¯”è¡¨
        performance_df = pd.DataFrame({
            model: {
                'MSE': result['MSE'],
                'MAE': result['MAE'],
                'RMSE': result['RMSE'],
                'RÂ²': result['RÂ²'],
                'MAPE(%)': result['MAPE'],
                'æ–¹å‘å‡†ç¡®ç‡(%)': result['Direction_Accuracy']
            }
            for model, result in results.items()
        }).T
        
        print("\n\næ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
        print(performance_df.round(4))
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = performance_df['RÂ²'].idxmax()
        print(f"\næœ€ä½³æ¨¡å‹ï¼ˆåŸºäºRÂ²ï¼‰: {best_model}")
        
        return results, performance_df
    
    def perform_shap_analysis(self):
        """æ‰§è¡ŒSHAPå¯è§£é‡Šæ€§åˆ†æ"""
        print("="*60)
        print("SHAPå¯è§£é‡Šæ€§åˆ†æ")
        print("="*60)
        
        if not self.models:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        
        # é€‰æ‹©æœ€ä½³çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡ŒSHAPåˆ†æ
        ml_models = ['RandomForest', 'GradientBoosting', 'XGBoost']
        available_ml_models = [name for name in ml_models if name in self.models]
        
        if not available_ml_models:
            print("æ²¡æœ‰å¯ç”¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡ŒSHAPåˆ†æ")
            return
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºRÂ²ï¼‰
        if not hasattr(self, 'predictions'):
            print("è¯·å…ˆè¿è¡Œæ¨¡å‹è¯„ä¼°")
            return
        
        ml_results = {name: result for name, result in self.predictions.items() 
                     if name in available_ml_models}
        
        if not ml_results:
            print("æ²¡æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹ç»“æœ")
            return
        
        best_ml_model = max(ml_results.keys(), 
                           key=lambda x: ml_results[x]['RÂ²'])
        model = self.models[best_ml_model]
        
        print(f"é€‰æ‹© {best_ml_model} æ¨¡å‹è¿›è¡ŒSHAPåˆ†æ...")
        
        # å‡†å¤‡SHAPåˆ†ææ•°æ®
        X_train = self.train_data['X_ml_train']
        X_test = self.train_data['X_ml_test']
        
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        print("åˆ›å»ºSHAPè§£é‡Šå™¨...")
        if best_ml_model in ['RandomForest', 'XGBoost']:
            explainer = shap.TreeExplainer(model)
        else:
            explainer = shap.Explainer(model, X_train[:100])  # ä½¿ç”¨æ ·æœ¬ä½œä¸ºèƒŒæ™¯
        
        # è®¡ç®—SHAPå€¼
        print("è®¡ç®—SHAPå€¼...")
        shap_values = explainer.shap_values(X_test)
        
        # å¦‚æœæ˜¯å¤šè¾“å‡ºï¼Œå–ç¬¬ä¸€ä¸ªè¾“å‡ºçš„SHAPå€¼
        if isinstance(shap_values, list):
            shap_values = shap_values[0]
        
        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        feature_importance = pd.DataFrame({
            'Feature': self.feature_names,
            'Importance': np.abs(shap_values).mean(axis=0)
        }).sort_values('Importance', ascending=False)
        
        print("\nç‰¹å¾é‡è¦æ€§æ’åº:")
        print(feature_importance.head(10))
        
        # ä¿å­˜SHAPåˆ†æç»“æœ
        self.shap_values = {
            'values': shap_values,
            'explainer': explainer,
            'feature_importance': feature_importance,
            'model_name': best_ml_model
        }
        
        return self.shap_values
    
    def create_visualizations(self, save_dir=None):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("="*60)
        print("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨")
        print("="*60)
        
        if save_dir is None:
            save_dir = self.output_dirs['pic']
        
        os.makedirs(save_dir, exist_ok=True)
        
        # 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
        if hasattr(self, 'predictions'):
            self._plot_model_performance(save_dir)
        
        # 2. é¢„æµ‹ç»“æœå¯¹æ¯”
        if hasattr(self, 'predictions'):
            self._plot_predictions(save_dir)
        
        # 3. SHAPåˆ†æå›¾è¡¨
        if hasattr(self, 'shap_values'):
            self._plot_shap_analysis(save_dir)
        
        # 4. è®­ç»ƒå†å²
        if hasattr(self, 'train_history'):
            self._plot_training_history(save_dir)
        
        print(f"æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {save_dir}")
    
    def _plot_model_performance(self, save_dir):
        """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾"""
        if not hasattr(self, 'predictions'):
            return
        
        # å‡†å¤‡æ•°æ®
        models = list(self.predictions.keys())
        metrics = ['MSE', 'MAE', 'RÂ²', 'MAPE', 'Direction_Accuracy']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()
        
        for i, metric in enumerate(metrics):
            if i < len(axes):
                values = [self.predictions[model][metric] for model in models]
                
                ax = axes[i]
                bars = ax.bar(models, values, alpha=0.7)
                ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')
                ax.set_ylabel(metric)
                ax.tick_params(axis='x', rotation=45)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, value in zip(bars, values):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{value:.3f}', ha='center', va='bottom')
                
                ax.grid(True, alpha=0.3)
        
        # ç§»é™¤å¤šä½™çš„å­å›¾
        for i in range(len(metrics), len(axes)):
            fig.delaxes(axes[i])
        
        plt.tight_layout()
        pic_file = os.path.join(save_dir, f'{self.run_name}_model_performance_comparison.png')
        plt.savefig(pic_file, dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_predictions(self, save_dir):
        """ç»˜åˆ¶é¢„æµ‹ç»“æœå¯¹æ¯”å›¾"""
        if not hasattr(self, 'predictions'):
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # é€‰æ‹©æœ€å¥½çš„å‡ ä¸ªæ¨¡å‹è¿›è¡Œå±•ç¤º
        models_to_show = list(self.predictions.keys())[:4]
        
        for i, model_name in enumerate(models_to_show):
            ax = axes[i//2, i%2]
            
            actual = self.predictions[model_name]['actual']
            predicted = self.predictions[model_name]['predictions']
            
            # åªæ˜¾ç¤ºæœ€å200ä¸ªç‚¹ä»¥æé«˜å¯è¯»æ€§
            if len(actual) > 200:
                actual = actual[-200:]
                predicted = predicted[-200:]
            
            ax.plot(actual, label='Actual', linewidth=2, alpha=0.8)
            ax.plot(predicted, label='Predicted', linewidth=2, alpha=0.8)
            
            ax.set_title(f'{model_name} Prediction Results', fontsize=14, fontweight='bold')
            ax.set_xlabel('Time Steps')
            ax.set_ylabel('Carbon Price')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ RÂ²ä¿¡æ¯
            r2 = self.predictions[model_name]['RÂ²']
            ax.text(0.05, 0.95, f'RÂ² = {r2:.4f}', 
                   transform=ax.transAxes, fontsize=12,
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        pic_file = os.path.join(save_dir, f'{self.run_name}_prediction_comparison.png')
        plt.savefig(pic_file, dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_shap_analysis(self, save_dir):
        """ç»˜åˆ¶SHAPåˆ†æå›¾è¡¨"""
        if not hasattr(self, 'shap_values'):
            return
        
        shap_vals = self.shap_values['values']
        X_test = self.train_data['X_ml_test']
        
        # 1. SHAP Summary Plot
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_vals, X_test, feature_names=self.feature_names, 
                         show=False)
        plt.title('SHAP Feature Importance Summary', fontsize=16, fontweight='bold')
        plt.tight_layout()
        pic_file = os.path.join(save_dir, f'{self.run_name}_shap_summary_plot.png')
        plt.savefig(pic_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        # 2. SHAP Bar Plot
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_vals, X_test, feature_names=self.feature_names,
                         plot_type="bar", show=False)
        plt.title('SHAP Feature Importance Bar Chart', fontsize=16, fontweight='bold')
        plt.tight_layout()
        pic_file = os.path.join(save_dir, f'{self.run_name}_shap_bar_plot.png')
        plt.savefig(pic_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        # 3. å‰å‡ ä¸ªé‡è¦ç‰¹å¾çš„ä¾èµ–å›¾
        top_features = self.shap_values['feature_importance'].head(4)['Feature'].tolist()
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.ravel()
        
        for i, feature in enumerate(top_features):
            if i < len(axes):
                feature_idx = self.feature_names.index(feature)
                
                plt.sca(axes[i])
                shap.dependence_plot(feature_idx, shap_vals, X_test,
                                   feature_names=self.feature_names, 
                                   show=False)
                axes[i].set_title(f'{feature} Dependence Plot', fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        pic_file = os.path.join(save_dir, f'{self.run_name}_shap_dependence_plots.png')
        plt.savefig(pic_file, dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_training_history(self, save_dir):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        if not hasattr(self, 'train_history'):
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        
        # LSTMè®­ç»ƒå†å²
        if 'lstm' in self.train_history:
            history = self.train_history['lstm']
            
            axes[0, 0].plot(history.history['loss'], label='Training Loss')
            axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')
            axes[0, 0].set_title('LSTM Loss Curve')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            axes[0, 1].plot(history.history['mae'], label='Training MAE')
            axes[0, 1].plot(history.history['val_mae'], label='Validation MAE')
            axes[0, 1].set_title('LSTM MAE Curve')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('MAE')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # Transformerè®­ç»ƒå†å²
        if 'transformer' in self.train_history:
            history = self.train_history['transformer']
            
            axes[1, 0].plot(history.history['loss'], label='Training Loss')
            axes[1, 0].plot(history.history['val_loss'], label='Validation Loss')
            axes[1, 0].set_title('Transformer Loss Curve')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Loss')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
            
            axes[1, 1].plot(history.history['mae'], label='Training MAE')
            axes[1, 1].plot(history.history['val_mae'], label='Validation MAE')
            axes[1, 1].set_title('Transformer MAE Curve')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('MAE')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        pic_file = os.path.join(save_dir, f'{self.run_name}_training_history.png')
        plt.savefig(pic_file, dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_report(self, save_path=None):
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        print("="*60)
        print("ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        print("="*60)
        
        # ç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æŠ¥å‘Š
        self._generate_detailed_text_report()
        
        # ç”Ÿæˆè¿è¡Œæ—¥å¿—
        self._generate_runtime_log()
        
        if save_path is None:
            save_path = os.path.join(self.output_dirs['excel'], f'{self.run_name}_report.xlsx')
        
        with pd.ExcelWriter(save_path, engine='openpyxl') as writer:
            
            # 1. æ•°æ®æ¦‚è¦
            if self.processed_data is not None:
                data_summary = self.processed_data.describe()
                data_summary.to_excel(writer, sheet_name='æ•°æ®æ¦‚è¦')
            
            # 2. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
            if hasattr(self, 'predictions'):
                performance_df = pd.DataFrame({
                    model: {
                        'MSE': result['MSE'],
                        'MAE': result['MAE'],
                        'RMSE': result['RMSE'],
                        'RÂ²': result['RÂ²'],
                        'MAPE(%)': result['MAPE'],
                        'æ–¹å‘å‡†ç¡®ç‡(%)': result['Direction_Accuracy']
                    }
                    for model, result in self.predictions.items()
                }).T
                performance_df.to_excel(writer, sheet_name='æ¨¡å‹æ€§èƒ½')
            
            # 3. ç‰¹å¾é‡è¦æ€§
            if hasattr(self, 'shap_values'):
                feature_importance = self.shap_values['feature_importance']
                feature_importance.to_excel(writer, sheet_name='ç‰¹å¾é‡è¦æ€§', index=False)
            
            # 4. é¢„æµ‹ç»“æœï¼ˆé€‰æ‹©æœ€ä½³æ¨¡å‹ï¼‰
            if hasattr(self, 'predictions'):
                best_model = max(self.predictions.keys(), 
                               key=lambda x: self.predictions[x]['RÂ²'])
                
                predictions_df = pd.DataFrame({
                    'å®é™…å€¼': self.predictions[best_model]['actual'],
                    'é¢„æµ‹å€¼': self.predictions[best_model]['predictions'],
                    'è¯¯å·®': (self.predictions[best_model]['actual'] - 
                             self.predictions[best_model]['predictions'])
                })
                predictions_df.to_excel(writer, sheet_name=f'{best_model}_é¢„æµ‹ç»“æœ', index=False)
            
            # 5. ç³»ç»Ÿé…ç½®ä¿¡æ¯
            config_df = pd.DataFrame([
                ['ç›®æ ‡åˆ—', self.config.get('target_column', 'carbon_price')],
                ['åºåˆ—é•¿åº¦', self.config.get('sequence_length', 60)],
                ['æµ‹è¯•é›†æ¯”ä¾‹', self.config.get('test_size', 0.2)],
                ['éšæœºç§å­', self.config.get('random_state', 42)],
                ['æ•°æ®ç‚¹æ•°é‡', len(self.processed_data) if self.processed_data is not None else 0],
                ['ç‰¹å¾æ•°é‡', len(self.feature_names) if hasattr(self, 'feature_names') else 0],
                ['ç”Ÿæˆæ—¶é—´', pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')]
            ], columns=['é…ç½®é¡¹', 'æ•°å€¼'])
            config_df.to_excel(writer, sheet_name='ç³»ç»Ÿé…ç½®', index=False)
            
            # 6. æ•°æ®è´¨é‡åˆ†æ
            if self.processed_data is not None:
                quality_df = pd.DataFrame([
                    ['æ•°æ®å®Œæ•´æ€§', f"{(1 - self.processed_data.isnull().sum().sum() / (len(self.processed_data) * len(self.processed_data.columns))) * 100:.1f}%"],
                    ['æ—¶é—´èŒƒå›´', f"{self.processed_data.index[0].strftime('%Y-%m-%d')} åˆ° {self.processed_data.index[-1].strftime('%Y-%m-%d')}"],
                    ['ä»·æ ¼æ³¢åŠ¨ç‡', f"{self.processed_data[self.config.get('target_column', 'carbon_price')].std():.4f}"],
                    ['ä»·æ ¼èŒƒå›´', f"{self.processed_data[self.config.get('target_column', 'carbon_price')].min():.2f} - {self.processed_data[self.config.get('target_column', 'carbon_price')].max():.2f}"],
                    ['æ•°æ®æ¥æº', self.data_source if hasattr(self, 'data_source') else 'ç¤ºä¾‹æ•°æ®']
                ], columns=['è´¨é‡æŒ‡æ ‡', 'æ•°å€¼'])
                quality_df.to_excel(writer, sheet_name='æ•°æ®è´¨é‡', index=False)
        
        print(f"ExcelæŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        print(f"è¯¦ç»†æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜åˆ°: {os.path.join(self.output_dirs['txt'], f'{self.run_name}_detailed_report.txt')}")
        print(f"è¿è¡Œæ—¥å¿—å·²ä¿å­˜åˆ°: {os.path.join(self.output_dirs['txt'], f'{self.run_name}_runtime_log.txt')}")
        
        # æ‰“å°ç®€è¦æŠ¥å‘Š
        self._print_summary_report()
    
    def _generate_detailed_text_report(self):
        """ç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æŠ¥å‘Š"""
        report_content = []
        report_content.append("=" * 80)
        report_content.append("ç¢³ä»·æ ¼é¢„æµ‹ç³»ç»Ÿè¯¦ç»†åˆ†ææŠ¥å‘Š")
        report_content.append("=" * 80)
        report_content.append(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_content.append("")
        
        # æ•°æ®æ¦‚è¦
        report_content.append("ğŸ“ˆ æ•°æ®ä¿¡æ¯:")
        if self.processed_data is not None:
            report_content.append(f"   â€¢ æ•°æ®ç‚¹æ•°é‡: {len(self.processed_data):,}")
            report_content.append(f"   â€¢ ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            report_content.append(f"   â€¢ æ—¶é—´èŒƒå›´: {self.processed_data.index[0].strftime('%Y-%m-%d')} åˆ° {self.processed_data.index[-1].strftime('%Y-%m-%d')}")
            target_col = self.config.get('target_column', 'carbon_price')
            if target_col in self.processed_data.columns:
                report_content.append(f"   â€¢ ç¢³ä»·æ ¼èŒƒå›´: {self.processed_data[target_col].min():.2f} - {self.processed_data[target_col].max():.2f}")
        report_content.append("")
        
        # æ¨¡å‹æ€§èƒ½
        if hasattr(self, 'predictions'):
            report_content.append("ğŸ† æ¨¡å‹æ€§èƒ½è¯¦æƒ…:")
            sorted_models = sorted(self.predictions.items(), 
                                 key=lambda x: x[1]['RÂ²'], reverse=True)
            
            for i, (model, result) in enumerate(sorted_models, 1):
                report_content.append(f"   {i}. {model}:")
                report_content.append(f"      â€¢ RÂ²: {result['RÂ²']:.4f}")
                report_content.append(f"      â€¢ RMSE: {result['RMSE']:.4f}")
                report_content.append(f"      â€¢ MAE: {result['MAE']:.4f}")
                report_content.append(f"      â€¢ MAPE: {result['MAPE']:.2f}%")
                report_content.append(f"      â€¢ æ–¹å‘å‡†ç¡®ç‡: {result['Direction_Accuracy']:.2f}%")
                
                # æ€§èƒ½è¯„ä¼°
                if result['RÂ²'] > 0.8:
                    performance_level = "ä¼˜ç§€"
                elif result['RÂ²'] > 0.6:
                    performance_level = "è‰¯å¥½"
                else:
                    performance_level = "å¾…æ”¹è¿›"
                report_content.append(f"      â€¢ æ€§èƒ½ç­‰çº§: {performance_level}")
                report_content.append("")
        
        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        if hasattr(self, 'shap_values'):
            report_content.append("ğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ:")
            top_features = self.shap_values['feature_importance'].head(10)
            for i, (_, row) in enumerate(top_features.iterrows(), 1):
                report_content.append(f"   {i:2d}. {row['Feature']:20s}: {row['Importance']:.6f}")
            report_content.append("")
        
        # ç³»ç»Ÿé…ç½®ä¿¡æ¯
        report_content.append("âš™ï¸ ç³»ç»Ÿé…ç½®:")
        report_content.append(f"   â€¢ ç›®æ ‡åˆ—: {self.config.get('target_column', 'carbon_price')}")
        report_content.append(f"   â€¢ åºåˆ—é•¿åº¦: {self.config.get('sequence_length', 60)}")
        report_content.append(f"   â€¢ æµ‹è¯•é›†æ¯”ä¾‹: {self.config.get('test_size', 0.2)}")
        report_content.append(f"   â€¢ éšæœºç§å­: {self.config.get('random_state', 42)}")
        report_content.append("")
        
        # æ¨¡å‹è¯¦ç»†é…ç½®
        if hasattr(self, 'predictions'):
            report_content.append("ğŸ”§ æ¨¡å‹é…ç½®è¯¦æƒ…:")
            if 'LSTM' in self.predictions:
                lstm_config = self.config.get('lstm_config', {})
                report_content.append(f"   â€¢ LSTMæ¨¡å‹:")
                report_content.append(f"     - éšè—å±‚å•å…ƒ: {lstm_config.get('hidden_units', 50)}")
                report_content.append(f"     - è®­ç»ƒè½®æ•°: {lstm_config.get('epochs', 100)}")
                report_content.append(f"     - æ‰¹é‡å¤§å°: {lstm_config.get('batch_size', 32)}")
            
            if 'Transformer' in self.predictions:
                transformer_config = self.config.get('transformer_config', {})
                report_content.append(f"   â€¢ Transformeræ¨¡å‹:")
                report_content.append(f"     - æ³¨æ„åŠ›å¤´æ•°: {transformer_config.get('num_heads', 8)}")
                report_content.append(f"     - æ¨¡å‹ç»´åº¦: {transformer_config.get('d_model', 64)}")
                report_content.append(f"     - è®­ç»ƒè½®æ•°: {transformer_config.get('epochs', 50)}")
            report_content.append("")
        
        # åº”ç”¨å»ºè®®
        report_content.append("ğŸ’¡ åº”ç”¨å»ºè®®:")
        report_content.append("   â€¢ å®šæœŸæ›´æ–°æ¨¡å‹ä»¥ä¿æŒé¢„æµ‹å‡†ç¡®æ€§")
        report_content.append("   â€¢ ç»“åˆSHAPåˆ†æç»“æœç†è§£é¢„æµ‹é€»è¾‘")
        report_content.append("   â€¢ åœ¨é‡å¤§å†³ç­–å‰è€ƒè™‘å¤šä¸ªæ¨¡å‹çš„é›†æˆç»“æœ")
        
        if hasattr(self, 'predictions'):
            best_model = max(self.predictions.keys(), 
                           key=lambda x: self.predictions[x]['RÂ²'])
            best_r2 = self.predictions[best_model]['RÂ²']
            
            if best_r2 > 0.8:
                report_content.append("   â€¢ æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼Œå¯ç”¨äºå®é™…é¢„æµ‹")
            elif best_r2 > 0.6:
                report_content.append("   â€¢ æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–")
            else:
                report_content.append("   â€¢ æ¨¡å‹æ€§èƒ½å¾…æå‡ï¼Œå»ºè®®å¢åŠ ç‰¹å¾æˆ–è°ƒæ•´æ¨¡å‹")
        
        report_content.append("   â€¢ ç›‘æ§é‡è¦ç‰¹å¾çš„å˜åŒ–è¶‹åŠ¿")
        report_content.append("   â€¢ ç»“åˆé¢†åŸŸçŸ¥è¯†ç†è§£æ¨¡å‹è¾“å‡º")
        report_content.append("")
        
        # æ•°æ®è´¨é‡è¯„ä¼°
        if self.processed_data is not None:
            missing_data = self.processed_data.isnull().sum().sum()
            target_col = self.config.get('target_column', 'carbon_price')
            price_volatility = self.processed_data[target_col].std() if target_col in self.processed_data.columns else 0
            
            report_content.append("ğŸ” æ•°æ®è´¨é‡è¯„ä¼°:")
            report_content.append(f"   â€¢ ç¼ºå¤±å€¼æ•°é‡: {missing_data}")
            data_completeness = (1 - missing_data / (len(self.processed_data) * len(self.processed_data.columns))) * 100
            report_content.append(f"   â€¢ æ•°æ®å®Œæ•´æ€§: {data_completeness:.1f}%")
            report_content.append(f"   â€¢ ä»·æ ¼æ³¢åŠ¨ç‡: {price_volatility:.2f}")
            report_content.append("")
        
        # é£é™©è­¦å‘Š
        report_content.append("âš ï¸ é£é™©æç¤º:")
        report_content.append("   â€¢ æ¨¡å‹é¢„æµ‹ä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®")
        report_content.append("   â€¢ ç¢³å¸‚åœºå—æ”¿ç­–å½±å“è¾ƒå¤§ï¼Œå­˜åœ¨ä¸ç¡®å®šæ€§")
        report_content.append("   â€¢ å»ºè®®ç»“åˆå¤šç§ä¿¡æ¯æºè¿›è¡Œç»¼åˆåˆ¤æ–­")
        report_content.append("   â€¢ æ¨¡å‹éœ€è¦å®šæœŸé‡æ–°è®­ç»ƒä»¥é€‚åº”å¸‚åœºå˜åŒ–")
        report_content.append("")
        
        report_content.append("=" * 80)
        report_content.append("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        report_content.append("=" * 80)
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        txt_file = os.path.join(self.output_dirs['txt'], f'{self.run_name}_detailed_report.txt')
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
    
    def _generate_runtime_log(self):
        """ç”Ÿæˆè¿è¡Œæ—¶æ—¥å¿—"""
        log_content = []
        log_content.append("=" * 80)
        log_content.append("ç¢³ä»·æ ¼é¢„æµ‹ç³»ç»Ÿè¿è¡Œæ—¥å¿—")
        log_content.append("=" * 80)
        log_content.append(f"è¿è¡Œæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        log_content.append(f"è¿è¡ŒID: {self.run_name}")
        log_content.append("")
        
        # ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
        import sys
        import platform
        log_content.append("ğŸ–¥ï¸ ç³»ç»Ÿç¯å¢ƒ:")
        log_content.append(f"   â€¢ Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
        log_content.append(f"   â€¢ æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
        log_content.append(f"   â€¢ å¤„ç†å™¨æ¶æ„: {platform.machine()}")
        log_content.append("")
        
        # ä¾èµ–åº“ä¿¡æ¯
        log_content.append("ğŸ“¦ ä¾èµ–åº“ç‰ˆæœ¬:")
        try:
            import pandas as pd_version
            log_content.append(f"   â€¢ pandas: {pd_version.__version__}")
        except:
            log_content.append("   â€¢ pandas: æœªçŸ¥ç‰ˆæœ¬")
        
        try:
            import numpy as np_version
            log_content.append(f"   â€¢ numpy: {np_version.__version__}")
        except:
            log_content.append("   â€¢ numpy: æœªçŸ¥ç‰ˆæœ¬")
        
        try:
            import sklearn
            log_content.append(f"   â€¢ scikit-learn: {sklearn.__version__}")
        except:
            log_content.append("   â€¢ scikit-learn: æœªçŸ¥ç‰ˆæœ¬")
        
        try:
            import tensorflow as tf
            log_content.append(f"   â€¢ tensorflow: {tf.__version__}")
        except:
            log_content.append("   â€¢ tensorflow: æœªå®‰è£…")
        
        try:
            import shap
            log_content.append(f"   â€¢ shap: {shap.__version__}")
        except:
            log_content.append("   â€¢ shap: æœªå®‰è£…")
        
        log_content.append("")
        
        # è¿è¡Œé…ç½®
        log_content.append("âš™ï¸ è¿è¡Œé…ç½®:")
        for key, value in self.config.items():
            log_content.append(f"   â€¢ {key}: {value}")
        log_content.append("")
        
        # æ•°æ®ä¿¡æ¯
        if self.processed_data is not None:
            log_content.append("ğŸ“Š æ•°æ®ä¿¡æ¯:")
            log_content.append(f"   â€¢ æ•°æ®æ¥æº: {getattr(self, 'data_source', 'ç¤ºä¾‹æ•°æ®')}")
            log_content.append(f"   â€¢ æ•°æ®å½¢çŠ¶: {self.processed_data.shape}")
            log_content.append(f"   â€¢ ç‰¹å¾åˆ—è¡¨: {list(self.feature_names)}")
            log_content.append(f"   â€¢ ç›®æ ‡å˜é‡: {self.config.get('target_column', 'carbon_price')}")
            log_content.append("")
        
        # æ¨¡å‹è®­ç»ƒä¿¡æ¯
        if hasattr(self, 'predictions'):
            log_content.append("ğŸ¤– æ¨¡å‹è®­ç»ƒä¿¡æ¯:")
            log_content.append(f"   â€¢ è®­ç»ƒæ¨¡å‹æ•°é‡: {len(self.predictions)}")
            log_content.append(f"   â€¢ è®­ç»ƒå®Œæˆæ¨¡å‹: {list(self.predictions.keys())}")
            
            best_model = max(self.predictions.keys(), 
                           key=lambda x: self.predictions[x]['RÂ²'])
            log_content.append(f"   â€¢ æœ€ä½³æ¨¡å‹: {best_model}")
            log_content.append(f"   â€¢ æœ€ä½³RÂ²: {self.predictions[best_model]['RÂ²']:.4f}")
            log_content.append("")
        
        # æ–‡ä»¶è¾“å‡ºä¿¡æ¯
        log_content.append("ğŸ“ ç”Ÿæˆæ–‡ä»¶:")
        log_content.append(f"   â€¢ {self.output_dirs['excel']}/{self.run_name}_report.xlsx - ExcelæŠ¥å‘Š")
        log_content.append(f"   â€¢ {self.output_dirs['txt']}/{self.run_name}_detailed_report.txt - è¯¦ç»†æ–‡æœ¬æŠ¥å‘Š")
        log_content.append(f"   â€¢ {self.output_dirs['txt']}/{self.run_name}_runtime_log.txt - è¿è¡Œæ—¥å¿—")
        log_content.append(f"   â€¢ {self.output_dirs['pic']}/ - å¯è§†åŒ–å›¾è¡¨ç›®å½•")
        log_content.append("")
        
        # è¿è¡ŒçŠ¶æ€
        log_content.append("âœ… è¿è¡ŒçŠ¶æ€: æˆåŠŸå®Œæˆ")
        log_content.append("")
        log_content.append("=" * 80)
        log_content.append("æ—¥å¿—è®°å½•å®Œæˆ")
        log_content.append("=" * 80)
        
        # ä¿å­˜è¿è¡Œæ—¥å¿—
        log_file = os.path.join(self.output_dirs['txt'], f'{self.run_name}_runtime_log.txt')
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(log_content))
    
    def _print_summary_report(self):
        """æ‰“å°ç®€è¦æŠ¥å‘Š"""
        print("\n" + "="*80)
        print(" " * 25 + "ç¢³ä»·æ ¼é¢„æµ‹ç³»ç»Ÿåˆ†ææŠ¥å‘Š")
        print("="*80)
        
        # æ•°æ®ä¿¡æ¯
        if self.processed_data is not None:
            print(f"\nğŸ“ˆ æ•°æ®ä¿¡æ¯:")
            print(f"   â€¢ æ•°æ®ç‚¹æ•°é‡: {len(self.processed_data):,}")
            print(f"   â€¢ ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            print(f"   â€¢ æ—¶é—´èŒƒå›´: {self.processed_data.index[0].strftime('%Y-%m-%d')} åˆ° {self.processed_data.index[-1].strftime('%Y-%m-%d')}")
        
        # æ¨¡å‹æ€§èƒ½
        if hasattr(self, 'predictions'):
            print(f"\nğŸ† æ¨¡å‹æ€§èƒ½æ’åï¼ˆæŒ‰RÂ²æ’åºï¼‰:")
            sorted_models = sorted(self.predictions.items(), 
                                 key=lambda x: x[1]['RÂ²'], reverse=True)
            
            for i, (model, result) in enumerate(sorted_models, 1):
                print(f"   {i}. {model}:")
                print(f"      â€¢ RÂ²: {result['RÂ²']:.4f}")
                print(f"      â€¢ RMSE: {result['RMSE']:.4f}")
                print(f"      â€¢ MAPE: {result['MAPE']:.2f}%")
                print(f"      â€¢ æ–¹å‘å‡†ç¡®ç‡: {result['Direction_Accuracy']:.2f}%")
        
        # å…³é”®ç‰¹å¾
        if hasattr(self, 'shap_values'):
            print(f"\nğŸ” å…³é”®å½±å“å› å­ï¼ˆæŒ‰SHAPé‡è¦æ€§ï¼‰:")
            top_features = self.shap_values['feature_importance'].head(5)
            for i, (_, row) in enumerate(top_features.iterrows(), 1):
                print(f"   {i}. {row['Feature']}: {row['Importance']:.4f}")
        
        # æ¨¡å‹å»ºè®®
        print(f"\nğŸ’¡ æ¨¡å‹åº”ç”¨å»ºè®®:")
        
        if hasattr(self, 'predictions'):
            best_model = max(self.predictions.keys(), 
                           key=lambda x: self.predictions[x]['RÂ²'])
            best_r2 = self.predictions[best_model]['RÂ²']
            
            if best_r2 > 0.8:
                print("   â€¢ æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼Œå¯ç”¨äºå®é™…é¢„æµ‹")
            elif best_r2 > 0.6:
                print("   â€¢ æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–")
            else:
                print("   â€¢ æ¨¡å‹æ€§èƒ½å¾…æå‡ï¼Œå»ºè®®å¢åŠ ç‰¹å¾æˆ–è°ƒæ•´æ¨¡å‹")
        
        print(f"   â€¢ å®šæœŸæ›´æ–°æ¨¡å‹ä»¥ä¿æŒé¢„æµ‹å‡†ç¡®æ€§")
        print(f"   â€¢ ç»“åˆSHAPåˆ†æç»“æœç†è§£é¢„æµ‹é€»è¾‘")
        print(f"   â€¢ åœ¨é‡å¤§å†³ç­–å‰è€ƒè™‘å¤šä¸ªæ¨¡å‹çš„é›†æˆç»“æœ")
        
        print("\n" + "="*80)
        print(" " * 30 + "åˆ†æå®Œæˆ")
        print("="*80)
    
    def run_complete_analysis(self, data_path=None):
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        ä¸€é”®å®Œæˆæ‰€æœ‰åˆ†æçš„ä¾¿æ·æ–¹æ³•
        =============================
        
        å‚æ•°:
            data_path: ä½ çš„æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
                      å¦‚æœä¸æä¾›ï¼Œç³»ç»Ÿå°†ä½¿ç”¨é»˜è®¤çš„{DEFAULT_DATA_FILE}æ–‡ä»¶
        
        å®Œæ•´åˆ†ææµç¨‹åŒ…æ‹¬ï¼š
        -------------------
        1. æ•°æ®åŠ è½½å’ŒéªŒè¯
        2. ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é¢„å¤„ç†
        3. å¤šæ¨¡å‹è®­ç»ƒï¼ˆLSTMã€Transformerã€éšæœºæ£®æ—ç­‰ï¼‰
        4. æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œå¯¹æ¯”
        5. SHAPå¯è§£é‡Šæ€§åˆ†æ
        6. å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ
        7. è¯¦ç»†åˆ†ææŠ¥å‘Šè¾“å‡º
        
        ä½¿ç”¨ä½ çš„æ•°æ®è¿›è¡Œåˆ†æï¼š
        -------------------------
        # æ–¹æ³•1ï¼šç›´æ¥æŒ‡å®šæ•°æ®æ–‡ä»¶
        # system = CarbonPricePredictionSystem()
        # system.run_complete_analysis('ä½ çš„æ•°æ®æ–‡ä»¶.xlsx')
        
        # æ–¹æ³•2ï¼šå…ˆé…ç½®å†åˆ†æ
        # config = {
        #     'target_column': 'ä½ çš„ç¢³ä»·æ ¼åˆ—å',
        #     'sequence_length': 60,
        #     'test_size': 0.2
        # }
        # system = CarbonPricePredictionSystem(config=config)
        # system.run_complete_analysis('æ•°æ®æ–‡ä»¶.xlsx')
        
        # æ–¹æ³•3ï¼šåˆ†æ­¥æ‰§è¡Œï¼ˆæ›´çµæ´»ï¼‰
        # system = CarbonPricePredictionSystem()
        # system.load_data('ä½ çš„æ•°æ®.xlsx')     # åŠ è½½æ•°æ®
        # system.preprocess_data()             # é¢„å¤„ç†
        # system.train_models()                # è®­ç»ƒæ¨¡å‹
        # system.evaluate_models()             # è¯„ä¼°æ€§èƒ½
        # system.perform_shap_analysis()       # SHAPåˆ†æ
        # system.create_visualizations()       # ç”Ÿæˆå›¾è¡¨
        # system.generate_report()             # ç”ŸæˆæŠ¥å‘Š
        
        è¾“å‡ºæ–‡ä»¶è¯´æ˜ï¼š
        ----------------
        - ExcelæŠ¥å‘Šï¼šåŒ…å«æ‰€æœ‰æ•°å€¼ç»“æœå’Œæ•°æ®è¡¨
        - è¯¦ç»†æ–‡æœ¬æŠ¥å‘Šï¼šå®Œæ•´çš„åˆ†æç»“æœè§£è¯»
        - è¿è¡Œæ—¥å¿—ï¼šç³»ç»Ÿé…ç½®å’Œè¿è¡Œä¿¡æ¯
        - å›¾è¡¨æ–‡ä»¶ï¼šæ¨¡å‹æ€§èƒ½ã€é¢„æµ‹ç»“æœã€SHAPåˆ†æç­‰å¯è§†åŒ–
        
        æ•°æ®è¦æ±‚æé†’ï¼š
        ----------------
        - ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼ˆæ—¥æœŸç´¢å¼• + æ•°å€¼åˆ—ï¼‰
        - æ•°æ®é‡å……è¶³ï¼ˆå»ºè®®1000+ä¸ªæ•°æ®ç‚¹ï¼‰
        - åŒ…å«è¶³å¤Ÿçš„å½±å“å› å­ï¼ˆå»ºè®®8-15ä¸ªå˜é‡ï¼‰
        - æ•°æ®è´¨é‡è‰¯å¥½ï¼ˆæ— å¼‚å¸¸å€¼ï¼Œå°‘é‡ç¼ºå¤±å€¼ï¼‰
        """
        print("ğŸš€ å¼€å§‹ç¢³ä»·æ ¼é¢„æµ‹å®Œæ•´åˆ†æ...\n")
        
        try:
            # 1. æ•°æ®æºå¤„ç†ï¼šæ ¹æ®é¡¹ç›®è®°å¿†ä½¿ç”¨é»˜è®¤æ–‡ä»¶
            if data_path:
                print(f"ğŸ“Š ä½¿ç”¨æŒ‡å®šçš„æ•°æ®æ–‡ä»¶: {data_path}")
                self.data_source = data_path
                self.load_data(data_path)
            else:
                # ä½¿ç”¨é»˜è®¤çš„æµ‹è¯•æ•°æ®æ–‡ä»¶
                default_data_path = DEFAULT_DATA_FILE
                print(f"ğŸ“Š æœªæŒ‡å®šæ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®: {default_data_path}")
                self.data_source = default_data_path
                
                # å°è¯•åŠ è½½é»˜è®¤æ•°æ®æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç¤ºä¾‹æ•°æ®
                try:
                    self.load_data(default_data_path)
                except (FileNotFoundError, IOError):
                    print(f"âš ï¸ æœªæ‰¾åˆ°é»˜è®¤æ•°æ®æ–‡ä»¶ {default_data_path}ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®...")
                    self.create_sample_data(save_path=default_data_path)
                    self.data_source = 'ç¤ºä¾‹æ•°æ®'
            
            # 2. æ•°æ®é¢„å¤„ç†
            self.preprocess_data()
            
            # 3. æ¨¡å‹è®­ç»ƒ
            self.train_models()
            
            # 4. æ¨¡å‹è¯„ä¼°
            self.evaluate_models()
            
            # 5. SHAPåˆ†æ
            self.perform_shap_analysis()
            
            # 6. åˆ›å»ºå¯è§†åŒ–
            self.create_visualizations()
            
            # 7. ç”ŸæˆæŠ¥å‘Š
            self.generate_report()
            
            print("\nâœ… å®Œæ•´åˆ†ææµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """
    ä¸»å‡½æ•°æ¼”ç¤º
    
    ğŸ“– å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®è¿è¡Œç³»ç»Ÿï¼š
    ================================
    
    ğŸ”§ æ–¹æ³•1ï¼šå¿«é€Ÿå¼€å§‹ï¼ˆæ¨èæ–°æ‰‹ï¼‰
    ----------------------------
    # ç›´æ¥æ›¿æ¢main()å‡½æ•°ä¸­çš„æ–‡ä»¶è·¯å¾„
    system = CarbonPricePredictionSystem()
    system.run_complete_analysis('ä½ çš„æ•°æ®æ–‡ä»¶.xlsx')  # æ”¹æˆä½ çš„æ–‡ä»¶è·¯å¾„
    
    ğŸ“‹ æ•°æ®å‡†å¤‡æ£€æŸ¥æ¸…å•ï¼š
    --------------------
    âœ… æ–‡ä»¶æ ¼å¼ï¼šExcel(.xlsx/.xls) æˆ– CSV(.csv)
    âœ… ç¬¬ä¸€åˆ—ï¼šæ—¥æœŸï¼ˆä½œä¸ºç´¢å¼•ï¼‰ï¼Œæ ¼å¼æ­£ç¡®
    âœ… æ•°æ®é‡ï¼šè‡³å°‘500è¡Œï¼Œæ¨è1000+è¡Œ
    âœ… ç¢³ä»·æ ¼åˆ—ï¼šåŒ…å«ç›®æ ‡å˜é‡
    âœ… å½±å“å› å­ï¼š8-15ä¸ªç›¸å…³å˜é‡
    âœ… æ•°æ®è´¨é‡ï¼šæ— å¼‚å¸¸å€¼ï¼Œç¼ºå¤±å€¼<5%
    âœ… æ—¶é—´è¿ç»­ï¼šæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
    
    ğŸ¯ é¢„æœŸè¾“å‡ºæ–‡ä»¶ï¼š
    ----------------
    â€¢ ExcelæŠ¥å‘Šï¼šåŒ…å«æ‰€æœ‰åˆ†æç»“æœå’Œæ•°æ®è¡¨
    â€¢ è¯¦ç»†æ–‡æœ¬æŠ¥å‘Šï¼šå®Œæ•´çš„åˆ†æç»“æœè§£è¯»  
    â€¢ è¿è¡Œæ—¥å¿—ï¼šç³»ç»Ÿé…ç½®å’Œè¿è¡Œä¿¡æ¯
    â€¢ å›¾è¡¨æ–‡ä»¶ï¼šæ¨¡å‹æ€§èƒ½ã€é¢„æµ‹ç»“æœã€SHAPåˆ†æç­‰å¯è§†åŒ–
    """
    print("ğŸŒ " + "="*60)
    print(" " * 20 + "ç¢³ä»·æ ¼é¢„æµ‹ç³»ç»Ÿ")
    print(" " * 15 + "LSTM + Transformer + SHAP åˆ†æ")
    print("="*60 + " ğŸŒ")
    
    # ğŸš€ ä½¿ç”¨è‡ªå·±æ•°æ®çš„ç¤ºä¾‹ï¼ˆå–æ¶ˆæ³¨é‡Šå¹¶ä¿®æ”¹è·¯å¾„ï¼‰ï¼š
    # =====================================================
    # 
    # æ–¹æ³•1ï¼šå¿«é€Ÿå¼€å§‹
    # system = CarbonPricePredictionSystem()
    # system.run_complete_analysis('ä½ çš„æ•°æ®æ–‡ä»¶.xlsx')  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
    # 
    # æ–¹æ³•2ï¼šè‡ªå®šä¹‰é…ç½®
    # my_config = {
    #     'target_column': 'ä½ çš„ç¢³ä»·æ ¼åˆ—å',  # å¦‚ï¼š'carbon_price', 'price', 'ç¢³ä»·æ ¼'ç­‰
    #     'sequence_length': 60,
    #     'test_size': 0.2
    # }
    # system = CarbonPricePredictionSystem(config=my_config)
    # system.run_complete_analysis('ä½ çš„æ•°æ®æ–‡ä»¶.xlsx')
    #
    # å½“å‰è¿è¡Œç¤ºä¾‹æ•°æ®æ¼”ç¤ºï¼š
    # =====================
    
    try:
        # åˆ›å»ºé¢„æµ‹ç³»ç»Ÿå®ä¾‹
        system = CarbonPricePredictionSystem()
        
        # ğŸš€ ä½¿ç”¨å…¨å±€å˜é‡å®šä¹‰çš„æµ‹è¯•æ•°æ®æ–‡ä»¶
        test_data_path = DEFAULT_DATA_FILE
        print(f"ğŸ“Š æ­£åœ¨ä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®æ–‡ä»¶: {test_data_path}")
        
        # è¿è¡Œå®Œæ•´åˆ†æï¼ˆä½¿ç”¨æŒ‡å®šçš„æµ‹è¯•æ•°æ®æ–‡ä»¶ï¼‰
        success = system.run_complete_analysis(test_data_path)
        
        if success:
            print("\nğŸ‰ ç¨‹åºæ‰§è¡ŒæˆåŠŸï¼")
            print("ğŸ“ ç”Ÿæˆæ–‡ä»¶:")
            print(f"   â€¢ outputs/reports/{system.run_name}_report.xlsx - å®Œæ•´Excelåˆ†ææŠ¥å‘Š")
            print(f"   â€¢ outputs/logs/{system.run_name}_detailed_report.txt - è¯¦ç»†æ–‡æœ¬åˆ†ææŠ¥å‘Š")
            print(f"   â€¢ outputs/logs/{system.run_name}_runtime_log.txt - ç³»ç»Ÿè¿è¡Œæ—¥å¿—")
            print(f"   â€¢ outputs/visualizations/ - å¯è§†åŒ–å›¾è¡¨ç›®å½•")
            print("\nğŸ”§ å¦‚ä½•ä½¿ç”¨ä½ è‡ªå·±çš„æ•°æ®:")
            print("   1. æ•°æ®æ ¼å¼ï¼šExcel(.xlsx)æˆ–CSV(.csv)ï¼Œç¬¬ä¸€åˆ—ä¸ºæ—¥æœŸ")
            print("   2. å¿…éœ€åˆ—ï¼šç¢³ä»·æ ¼åˆ—ï¼ˆåˆ—åå¯ä¸ºcarbon_priceã€priceã€ç¢³ä»·æ ¼ç­‰ï¼‰")
            print("   3. æ¨èåˆ—ï¼šGDPã€å·¥ä¸šæŒ‡æ•°ã€èƒ½æºä»·æ ¼ç­‰å½±å“å› å­ï¼ˆ8-15ä¸ªï¼‰")
            print("   4. æ•°æ®é‡ï¼šå»ºè®®1000+ä¸ªæ•°æ®ç‚¹ï¼Œæ—¶é—´è·¨åº¦3å¹´ä»¥ä¸Š")
            print("\nğŸ’» ä»£ç ç¤ºä¾‹:")
            print("   # åŸºæœ¬ç”¨æ³•")
            print("   system = CarbonPricePredictionSystem()")
            print("   system.run_complete_analysis('ä½ çš„æ•°æ®æ–‡ä»¶.xlsx')")
            print("\n   # è‡ªå®šä¹‰é…ç½®")
            print("   config = {'target_column': 'ä½ çš„ç¢³ä»·æ ¼åˆ—å'}")
            print("   system = CarbonPricePredictionSystem(config=config)")
            print("   system.run_complete_analysis('æ•°æ®æ–‡ä»¶.xlsx')")
        else:
            print("\nâš ï¸ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    except ImportError as e:
        print(f"\nâš ï¸ æ— æ³•å¯¼å…¥å®Œæ•´ç³»ç»Ÿ ({str(e)})ï¼Œè¿è¡ŒåŸºç¡€ç‰ˆæœ¬...")
        
        # è¿è¡ŒåŸºç¡€ç‰ˆæœ¬
        from carbon_test import SimpleCarbonPrediction
        
        print("\nğŸ“Š åŸºç¡€ç‰ˆæœ¬æ¼”ç¤º")
        print("="*40)
        
        # åˆ›å»ºåŸºç¡€ç‰ˆæœ¬ç³»ç»Ÿ
        basic_system = SimpleCarbonPrediction()
        
        # è¿è¡ŒåŸºç¡€ç‰ˆæœ¬åˆ†æ
        basic_system.run_analysis()
        
        print("\nâœ… åŸºç¡€ç‰ˆæœ¬æ¼”ç¤ºæˆåŠŸï¼")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
