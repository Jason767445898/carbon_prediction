#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¢³ä»·æ ¼é¢„æµ‹ç³»ç»Ÿ - LSTM + Attentionæœºåˆ¶
ä½¿ç”¨ data.DTA æ•°æ®æ–‡ä»¶ï¼Œé‡‡ç”¨ LSTM å’Œ Attention æœºåˆ¶çš„ç»“åˆæ¥é¢„æµ‹ç¢³ä»·æ ¼
è¾“å‡ºåŒ…æ‹¬ï¼šExcelè¡¨æ ¼ã€TXTæ–‡æ¡£ã€å›¾ç‰‡ç­‰åˆ†æžç»“æžœ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
from datetime import datetime
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# SHAPåˆ†æž
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("âš ï¸ SHAP not available, skipping interpretability analysis")

warnings.filterwarnings('ignore')

# è®¾ç½®è‹±æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
tf.random.set_seed(42)

# ============================================================================
# é…ç½®å‚æ•°
# ============================================================================

CONFIG = {
    'data_file': 'data.dta',
    'target_column': 'carbon_price_hb_ea',  # ç¢³ä»·æ ¼åˆ—
    'sequence_length': 30,  # åºåˆ—é•¿åº¦
    'test_size': 0.2,
    'validation_size': 0.1,
    'epochs': 100,
    'batch_size': 16,
    'learning_rate': 0.001,
    'lstm_units': 64,
    'attention_dim': 32,
    'dropout_rate': 0.2,
}

# è¾“å‡ºç›®å½•
OUTPUT_DIR = 'outputs'
for sub_dir in ['logs', 'reports', 'visualizations']:
    os.makedirs(os.path.join(OUTPUT_DIR, sub_dir), exist_ok=True)

# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def create_attention_layer(input_tensor, attention_dim):
    """åˆ›å»ºAttentionå±‚"""
    # Attention æƒé‡è®¡ç®—
    attention = layers.Dense(attention_dim, activation='tanh')(input_tensor)
    attention = layers.Dense(1, activation='sigmoid')(attention)
    attention = layers.Reshape((input_tensor.shape[1], 1))(attention)
    
    # åº”ç”¨attentionæƒé‡
    output = input_tensor * attention
    output = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(output)
    
    return output

def build_lstm_attention_model(sequence_length, n_features, lstm_units, attention_dim):
    """
    æž„å»º LSTM + Attention ç»„åˆæ¨¡åž‹
    """
    inputs = layers.Input(shape=(sequence_length, n_features))
    
    # LSTMå±‚
    lstm_out = layers.LSTM(lstm_units, return_sequences=True, 
                          dropout=CONFIG['dropout_rate'])(inputs)
    lstm_out = layers.LSTM(lstm_units//2, return_sequences=True,
                          dropout=CONFIG['dropout_rate'])(lstm_out)
    
    # Attentionå±‚
    attention_out = create_attention_layer(lstm_out, attention_dim)
    
    # å…¨è¿žæŽ¥å±‚
    dense = layers.Dense(32, activation='relu')(attention_out)
    dense = layers.Dropout(CONFIG['dropout_rate'])(dense)
    dense = layers.Dense(16, activation='relu')(dense)
    
    # è¾“å‡ºå±‚
    outputs = layers.Dense(1)(dense)
    
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=CONFIG['learning_rate']),
                  loss='mse',
                  metrics=['mae', 'mse'])
    
    return model

# ============================================================================
# ä¸»ç³»ç»Ÿç±»
# ============================================================================

class LSTMAttentionCarbonPrediction:
    """LSTM + Attention ç¢³ä»·æ ¼é¢„æµ‹ç³»ç»Ÿ"""
    
    def __init__(self):
        self.data = None
        self.processed_data = None
        self.model = None
        self.history = None
        self.predictions = None
        self.actual_values = None
        self.scaler_X = MinMaxScaler()
        self.scaler_y = MinMaxScaler()
        self.feature_names = []
        self.run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.results = {}
        self.shap_values = None
        self.rf_model = None  # ç”¨äºŽSHAPåˆ†æžçš„éšæœºæ£®æž—æ¨¡åž‹
        
    def load_data(self, file_path):
        """åŠ è½½æ•°æ®"""
        print(f"ðŸ“Š åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è¯»å– Stata æ–‡ä»¶
        self.data = pd.read_stata(file_path)
        
        # è½¬æ¢æ—¥æœŸåˆ—
        self.data['date'] = pd.to_datetime(self.data['date'])
        self.data.set_index('date', inplace=True)
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"   â€¢ æ•°æ®å½¢çŠ¶: {self.data.shape}")
        print(f"   â€¢ æ—¶é—´èŒƒå›´: {self.data.index[0]} åˆ° {self.data.index[-1]}")
        print(f"   â€¢ åˆ—æ•°: {len(self.data.columns)}")
        
        return self.data
    
    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        print("\nðŸ”§ Data Preprocessing...")
        
        df = self.data.copy()
        target = CONFIG['target_column']
        
        # æ£€æŸ¥ç›®æ ‡åˆ—
        if target not in df.columns:
            raise ValueError(f"Target column '{target}' not found")
        
        print(f"   â€¢ Original shape: {df.shape}")
        print(f"   â€¢ Missing values before cleaning: {df.isnull().sum().sum()}")
        
        # 1. åˆ é™¤ç©ºå€¼æ¯”ä¾‹è¿‡é«˜çš„åˆ—ï¼ˆè¶…è¿‡80%ï¼‰
        null_ratio = df.isnull().sum() / len(df)
        cols_to_drop = null_ratio[null_ratio > 0.8].index.tolist()
        if cols_to_drop:
            print(f"   â€¢ Dropping columns with >80% missing: {cols_to_drop}")
            df = df.drop(columns=cols_to_drop)
        
        # 2. åˆ é™¤å…¨ä¸ºNaNçš„åˆ—
        df = df.dropna(axis=1, how='all')
        
        # 3. åˆ é™¤ç©ºå€¼æ¯”ä¾‹è¿‡é«˜çš„è¡Œï¼ˆè¶…è¿‡50%ï¼‰
        row_null_ratio = df.isnull().sum(axis=1) / len(df.columns)
        rows_to_keep = row_null_ratio <= 0.5
        df = df[rows_to_keep]
        print(f"   â€¢ Rows removed: {(~rows_to_keep).sum()}")
        
        # 4. å¤„ç†å‰©ä½™ç¼ºå¤±å€¼ - ä½¿ç”¨å¤šå±‚æ’å€¼
        for col in df.columns:
            if df[col].isnull().any():
                # ç¬¬ä¸€å±‚ï¼šçº¿æ€§æ’å€¼
                df[col] = df[col].interpolate(method='linear', limit_direction='both')
                # ç¬¬äºŒå±‚ï¼šå¡«å……å‰©ä½™ç¼ºå¤±å€¼
                if df[col].isnull().any():
                    df[col] = df[col].fillna(df[col].mean())
                # ç¬¬ä¸‰å±‚ï¼šæœ€åŽçš„ä¿é™©
                if df[col].isnull().any():
                    df[col] = df[col].fillna(0)
        
        print(f"   â€¢ Missing values after cleaning: {df.isnull().sum().sum()}")
        
        # ç§»é™¤æ— ç©·å¤§
        df = df.replace([np.inf, -np.inf], np.nan)
        for col in df.columns:
            if df[col].isnull().any():
                df[col] = df[col].fillna(df[col].mean())
        
        print(f"   â€¢ Final shape: {df.shape}")
        
        # ç‰¹å¾å·¥ç¨‹
        df['price_return'] = df[target].pct_change()
        df['price_diff'] = df[target].diff()
        
        # ç§»åŠ¨å¹³å‡
        for window in [5, 10, 20]:
            df[f'ma_{window}'] = df[target].rolling(window, min_periods=1).mean()
        
        # æ³¢åŠ¨çŽ‡
        for window in [7, 14]:
            df[f'volatility_{window}'] = df[target].rolling(window, min_periods=1).std()
        
        # é€‰æ‹©ç‰¹å¾ï¼ˆæŽ’é™¤ç›®æ ‡åˆ—å’Œé¢å¤–è¡ç”Ÿåˆ—ï¼‰
        feature_cols = [col for col in df.columns 
                       if col != target and col not in ['price_return', 'price_diff']]
        
        self.feature_names = feature_cols
        self.processed_data = df
        
        print(f"âœ… Data Preprocessing Completed")
        print(f"   â€¢ Number of features: {len(self.feature_names)}")
        print(f"   â€¢ Data shape: {df.shape}")
        
        return df
    
    def create_sequences(self, data, feature_cols, target_col, seq_length):
        """åˆ›å»ºåºåˆ—æ•°æ®"""
        X, y = [], []
        
        for i in range(len(data) - seq_length):
            seq_X = data[feature_cols].iloc[i:i+seq_length].values
            seq_y = data[target_col].iloc[i+seq_length]
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«NaNæˆ–Inf
            if not (np.isnan(seq_X).any() or np.isinf(seq_X).any() or 
                   np.isnan(seq_y) or np.isinf(seq_y)):
                X.append(seq_X)
                y.append(seq_y)
        
        return np.array(X), np.array(y)
    
    def split_and_scale_data(self):
        """åˆ†å‰²å¹¶æ ‡å‡†åŒ–æ•°æ®"""
        print("\nðŸ“Š æ•°æ®åˆ†å‰²å’Œæ ‡å‡†åŒ–...")
        
        df = self.processed_data
        target = CONFIG['target_column']
        seq_len = CONFIG['sequence_length']
        
        # åˆ›å»ºåºåˆ—
        X, y = self.create_sequences(df, self.feature_names, target, seq_len)
        
        # å†æ¬¡æ£€æŸ¥NaNå€¼
        valid_idx = ~(np.isnan(X).any(axis=(1, 2)) | np.isnan(y))
        X = X[valid_idx]
        y = y[valid_idx]
        
        print(f"   â€¢ åºåˆ—æ•°é‡: {len(X)}")
        print(f"   â€¢ æ£€æŸ¥NaNå€¼: {np.isnan(X).any() or np.isnan(y).any()}")
        
        # æ—¶é—´åºåˆ—åˆ†å‰²
        n = len(X)
        train_size = int(n * (1 - CONFIG['test_size'] - CONFIG['validation_size']))
        val_size = int(n * (1 - CONFIG['test_size']))
        
        X_train, y_train = X[:train_size], y[:train_size]
        X_val, y_val = X[train_size:val_size], y[train_size:val_size]
        X_test, y_test = X[val_size:], y[val_size:]
        
        print(f"   â€¢ è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"   â€¢ éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        print(f"   â€¢ æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        
        # æ ‡å‡†åŒ–ç‰¹å¾ - ä½¿ç”¨æ›´ç¨³å¥çš„æ–¹æ³•
        X_train_flat = X_train.reshape(-1, X_train.shape[-1])
        X_train_flat = np.nan_to_num(X_train_flat, nan=0.0, posinf=0.0, neginf=0.0)
        X_train_flat = self.scaler_X.fit_transform(X_train_flat)
        X_train = X_train_flat.reshape(X_train.shape)
        
        X_val_flat = X_val.reshape(-1, X_val.shape[-1])
        X_val_flat = np.nan_to_num(X_val_flat, nan=0.0, posinf=0.0, neginf=0.0)
        X_val_flat = self.scaler_X.transform(X_val_flat)
        X_val = X_val_flat.reshape(X_val.shape)
        
        X_test_flat = X_test.reshape(-1, X_test.shape[-1])
        X_test_flat = np.nan_to_num(X_test_flat, nan=0.0, posinf=0.0, neginf=0.0)
        X_test_flat = self.scaler_X.transform(X_test_flat)
        X_test = X_test_flat.reshape(X_test.shape)
        
        # æ ‡å‡†åŒ–ç›®æ ‡å˜é‡
        y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)
        y_val = np.nan_to_num(y_val, nan=0.0, posinf=0.0, neginf=0.0)
        y_test = np.nan_to_num(y_test, nan=0.0, posinf=0.0, neginf=0.0)
        
        y_train = self.scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val = self.scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        y_test = self.scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        
        print(f"âœ… æ•°æ®åˆ†å‰²å’Œæ ‡å‡†åŒ–å®Œæˆ")
        
        return X_train, y_train, X_val, y_val, X_test, y_test
    
    def train_model(self, X_train, y_train, X_val, y_val):
        """è®­ç»ƒæ¨¡åž‹"""
        print("\nðŸ¤– è®­ç»ƒ LSTM + Attention æ¨¡åž‹...")
        
        n_features = X_train.shape[2]
        
        # æž„å»ºæ¨¡åž‹
        self.model = build_lstm_attention_model(
            sequence_length=CONFIG['sequence_length'],
            n_features=n_features,
            lstm_units=CONFIG['lstm_units'],
            attention_dim=CONFIG['attention_dim']
        )
        
        print("\næ¨¡åž‹æž¶æž„:")
        self.model.summary()
        
        # è®­ç»ƒå›žè°ƒ
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=20, 
                         restore_best_weights=True, verbose=1),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, 
                             patience=10, min_lr=1e-7, verbose=1)
        ]
        
        # è®­ç»ƒ
        self.history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=CONFIG['epochs'],
            batch_size=CONFIG['batch_size'],
            callbacks=callbacks,
            verbose=1
        )
        
        print("âœ… æ¨¡åž‹è®­ç»ƒå®Œæˆ")
        
        return self.model
    
    def evaluate_model(self, X_test, y_test):
        """è¯„ä¼°æ¨¡åž‹"""
        print("\nðŸ“ˆ è¯„ä¼°æ¨¡åž‹æ€§èƒ½...")
        
        # é¢„æµ‹
        y_pred_scaled = self.model.predict(X_test, verbose=0)
        
        # åæ ‡å‡†åŒ–
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled).flatten()
        y_true = self.scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
        
        # è®¡ç®—æŒ‡æ ‡
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        
        # æ–¹å‘å‡†ç¡®çŽ‡
        direction_acc = np.mean(
            np.sign(y_pred[1:] - y_pred[:-1]) == 
            np.sign(y_true[1:] - y_true[:-1])
        ) * 100
        
        self.predictions = y_pred
        self.actual_values = y_true
        
        print(f"   â€¢ MSE: {mse:.4f}")
        print(f"   â€¢ MAE: {mae:.4f}")
        print(f"   â€¢ RMSE: {rmse:.4f}")
        print(f"   â€¢ RÂ²: {r2:.4f}")
        print(f"   â€¢ MAPE: {mape:.2f}%")
        print(f"   â€¢ æ–¹å‘å‡†ç¡®çŽ‡: {direction_acc:.2f}%")
        
        self.results = {
            'MSE': mse,
            'MAE': mae,
            'RMSE': rmse,
            'R2': r2,
            'MAPE': mape,
            'Direction_Accuracy': direction_acc
        }
        
        print("âœ… Model Evaluation Completed")
        
        return self.results
    
    def perform_shap_analysis(self, X_train_ml, y_train_ml, X_test_ml):
        """æ‰§è¡ŒSHAPå¯è§£é‡Šæ€§åˆ†æž"""
        if not SHAP_AVAILABLE:
            print("\nâš ï¸ SHAP not installed, skipping interpretability analysis")
            return None
        
        print("\nðŸ” Performing SHAP Analysis...")
        
        # è®­ç»ƒéšæœºæ£®æž—æ¨¡åž‹ç”¨äºŽSHAPåˆ†æž
        print("   â€¢ Training Random Forest for SHAP...")
        self.rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        self.rf_model.fit(X_train_ml, y_train_ml)
        
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        print("   â€¢ Creating SHAP explainer...")
        explainer = shap.TreeExplainer(self.rf_model)
        
        # è®¡ç®—SHAPå€¼ï¼ˆä½¿ç”¨æµ‹è¯•é›†æ ·æœ¬ï¼‰
        print("   â€¢ Calculating SHAP values...")
        shap_values = explainer.shap_values(X_test_ml[:100])  # ä½¿ç”¨å‰100ä¸ªæ ·æœ¬
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'Feature': self.feature_names,
            'Importance': np.abs(shap_values).mean(axis=0)
        }).sort_values('Importance', ascending=False)
        
        print(f"\n   Top 10 Important Features:")
        for idx, row in feature_importance.head(10).iterrows():
            print(f"      {row['Feature']:30s}: {row['Importance']:.6f}")
        
        self.shap_values = {
            'values': shap_values,
            'explainer': explainer,
            'feature_importance': feature_importance,
            'X_test_sample': X_test_ml[:100]
        }
        
        print("\nâœ… SHAP Analysis Completed")
        
        return self.shap_values
    
    def create_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆè‹±æ–‡ï¼‰"""
        print("\nðŸŽ¨ Generating Visualizations...")
        
        pic_dir = os.path.join(OUTPUT_DIR, 'visualizations')
        
        # 1. Training History
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        axes[0].plot(self.history.history['loss'], label='Training Loss')
        axes[0].plot(self.history.history['val_loss'], label='Validation Loss')
        axes[0].set_title('Model Loss Curve', fontsize=12, fontweight='bold')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        axes[1].plot(self.history.history['mae'], label='Training MAE')
        axes[1].plot(self.history.history['val_mae'], label='Validation MAE')
        axes[1].set_title('Model MAE Curve', fontsize=12, fontweight='bold')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('MAE')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(pic_dir, f'{self.run_timestamp}_training_history.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Prediction Results
        fig, ax = plt.subplots(figsize=(14, 6))
        
        show_points = min(300, len(self.actual_values))
        
        ax.plot(self.actual_values[-show_points:], label='Actual', linewidth=2)
        ax.plot(self.predictions[-show_points:], label='Predicted', linewidth=2)
        ax.fill_between(range(show_points), 
                        self.actual_values[-show_points:],
                        self.predictions[-show_points:],
                        alpha=0.2)
        ax.set_title(f'Carbon Price Prediction (Last {show_points} Data Points)', 
                    fontsize=12, fontweight='bold')
        ax.set_xlabel('Time Steps')
        ax.set_ylabel('Carbon Price')
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3)
        
        textstr = f'R\u00b2 = {self.results["R2"]:.4f}\nRMSE = {self.results["RMSE"]:.4f}\nMAPE = {self.results["MAPE"]:.2f}%'
        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,
               verticalalignment='top', bbox=dict(boxstyle='round', 
               facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(os.path.join(pic_dir, f'{self.run_timestamp}_predictions.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Residual Analysis
        residuals = self.actual_values - self.predictions
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        axes[0].plot(residuals, label='Residuals', alpha=0.7)
        axes[0].axhline(y=0, color='r', linestyle='--', alpha=0.5)
        axes[0].set_title('Prediction Residuals', fontsize=12, fontweight='bold')
        axes[0].set_xlabel('Time Steps')
        axes[0].set_ylabel('Residuals')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        axes[1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)
        axes[1].set_title('Residual Distribution', fontsize=12, fontweight='bold')
        axes[1].set_xlabel('Residual Value')
        axes[1].set_ylabel('Frequency')
        axes[1].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig(os.path.join(pic_dir, f'{self.run_timestamp}_residuals.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. SHAP Visualizations
        if self.shap_values is not None and SHAP_AVAILABLE:
            print("   â€¢ Creating SHAP visualizations...")
            
            # SHAP Summary Plot
            plt.figure(figsize=(10, 8))
            shap.summary_plot(
                self.shap_values['values'], 
                self.shap_values['X_test_sample'],
                feature_names=self.feature_names,
                show=False
            )
            plt.title('SHAP Feature Importance Summary', fontsize=14, fontweight='bold', pad=20)
            plt.tight_layout()
            plt.savefig(os.path.join(pic_dir, f'{self.run_timestamp}_shap_summary.png'),
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            # SHAP Bar Plot
            plt.figure(figsize=(10, 8))
            shap.summary_plot(
                self.shap_values['values'],
                self.shap_values['X_test_sample'],
                feature_names=self.feature_names,
                plot_type="bar",
                show=False
            )
            plt.title('SHAP Feature Importance (Bar)', fontsize=14, fontweight='bold', pad=20)
            plt.tight_layout()
            plt.savefig(os.path.join(pic_dir, f'{self.run_timestamp}_shap_bar.png'),
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            # Feature Importance Comparison
            fig, ax = plt.subplots(figsize=(10, 6))
            top_features = self.shap_values['feature_importance'].head(15)
            ax.barh(range(len(top_features)), top_features['Importance'])
            ax.set_yticks(range(len(top_features)))
            ax.set_yticklabels(top_features['Feature'])
            ax.set_xlabel('Mean |SHAP Value|')
            ax.set_title('Top 15 Important Features (SHAP)', fontsize=12, fontweight='bold')
            ax.grid(True, alpha=0.3, axis='x')
            plt.tight_layout()
            plt.savefig(os.path.join(pic_dir, f'{self.run_timestamp}_feature_importance.png'),
                       dpi=300, bbox_inches='tight')
            plt.close()
        
        print(f"âœ… Visualizations saved to: {pic_dir}")
    
    def generate_excel_report(self):
        """ç”ŸæˆExcelæŠ¥å‘Š"""
        print("\nðŸ“Š ç”ŸæˆExcelæŠ¥å‘Š...")
        
        excel_path = os.path.join(OUTPUT_DIR, 'reports', 
                                 f'{self.run_timestamp}_carbon_prediction_report.xlsx')
        
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            
            # 1. æ¨¡åž‹æ€§èƒ½æŒ‡æ ‡
            metrics_df = pd.DataFrame([
                {'æŒ‡æ ‡': 'MSE', 'æ•°å€¼': f"{self.results['MSE']:.6f}"},
                {'æŒ‡æ ‡': 'MAE', 'æ•°å€¼': f"{self.results['MAE']:.6f}"},
                {'æŒ‡æ ‡': 'RMSE', 'æ•°å€¼': f"{self.results['RMSE']:.6f}"},
                {'æŒ‡æ ‡': 'RÂ²', 'æ•°å€¼': f"{self.results['R2']:.6f}"},
                {'æŒ‡æ ‡': 'MAPE(%)', 'æ•°å€¼': f"{self.results['MAPE']:.2f}"},
                {'æŒ‡æ ‡': 'æ–¹å‘å‡†ç¡®çŽ‡(%)', 'æ•°å€¼': f"{self.results['Direction_Accuracy']:.2f}"}
            ])
            metrics_df.to_excel(writer, sheet_name='æ¨¡åž‹æ€§èƒ½æŒ‡æ ‡', index=False)
            
            # 2. é¢„æµ‹ç»“æžœ
            predictions_df = pd.DataFrame({
                'å®žé™…å€¼': self.actual_values,
                'é¢„æµ‹å€¼': self.predictions,
                'è¯¯å·®': self.actual_values - self.predictions,
                'è¯¯å·®çŽ‡(%)': (np.abs(self.actual_values - self.predictions) / 
                            self.actual_values) * 100
            })
            predictions_df.to_excel(writer, sheet_name='é¢„æµ‹ç»“æžœ', index=False)
            
            # 3. æ•°æ®ç»Ÿè®¡
            data_stats = pd.DataFrame([
                {'ç»Ÿè®¡é¡¹': 'æ•°æ®æ€»æ•°', 'æ•°å€¼': len(self.actual_values)},
                {'ç»Ÿè®¡é¡¹': 'å®žé™…å€¼å‡å€¼', 'æ•°å€¼': f"{self.actual_values.mean():.4f}"},
                {'ç»Ÿè®¡é¡¹': 'å®žé™…å€¼æ ‡å‡†å·®', 'æ•°å€¼': f"{self.actual_values.std():.4f}"},
                {'ç»Ÿè®¡é¡¹': 'å®žé™…å€¼æœ€å°å€¼', 'æ•°å€¼': f"{self.actual_values.min():.4f}"},
                {'ç»Ÿè®¡é¡¹': 'å®žé™…å€¼æœ€å¤§å€¼', 'æ•°å€¼': f"{self.actual_values.max():.4f}"},
                {'ç»Ÿè®¡é¡¹': 'é¢„æµ‹å€¼å‡å€¼', 'æ•°å€¼': f"{self.predictions.mean():.4f}"},
                {'ç»Ÿè®¡é¡¹': 'é¢„æµ‹å€¼æ ‡å‡†å·®', 'æ•°å€¼': f"{self.predictions.std():.4f}"},
            ])
            data_stats.to_excel(writer, sheet_name='æ•°æ®ç»Ÿè®¡', index=False)
            
            # 4. ç³»ç»Ÿé…ç½®
            config_df = pd.DataFrame([
                {'é…ç½®é¡¹': 'åºåˆ—é•¿åº¦', 'æ•°å€¼': CONFIG['sequence_length']},
                {'é…ç½®é¡¹': 'è®­ç»ƒè½®æ•°', 'æ•°å€¼': CONFIG['epochs']},
                {'é…ç½®é¡¹': 'æ‰¹æ¬¡å¤§å°', 'æ•°å€¼': CONFIG['batch_size']},
                {'é…ç½®é¡¹': 'LSTMå•å…ƒæ•°', 'æ•°å€¼': CONFIG['lstm_units']},
                {'é…ç½®é¡¹': 'Attentionç»´åº¦', 'æ•°å€¼': CONFIG['attention_dim']},
                {'é…ç½®é¡¹': 'å­¦ä¹ çŽ‡', 'æ•°å€¼': CONFIG['learning_rate']},
                {'é…ç½®é¡¹': 'æµ‹è¯•é›†æ¯”ä¾‹', 'æ•°å€¼': CONFIG['test_size']},
            ])
            config_df.to_excel(writer, sheet_name='ç³»ç»Ÿé…ç½®', index=False)
        
        print(f"âœ… ExcelæŠ¥å‘Šå·²ä¿å­˜åˆ°: {excel_path}")
        return excel_path
    
    def generate_text_report(self):
        """ç”ŸæˆTXTæ–‡æœ¬æŠ¥å‘Š"""
        print("\nðŸ“„ ç”ŸæˆTXTæ–‡æœ¬æŠ¥å‘Š...")
        
        txt_path = os.path.join(OUTPUT_DIR, 'logs',
                               f'{self.run_timestamp}_analysis_report.txt')
        
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ç¢³ä»·æ ¼é¢„æµ‹ç³»ç»Ÿåˆ†æžæŠ¥å‘Š (LSTM + Attention)")
        report_lines.append("=" * 80)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # æ•°æ®ä¿¡æ¯
        report_lines.append("ðŸ“Š æ•°æ®ä¿¡æ¯:")
        report_lines.append(f"   â€¢ æ•°æ®æº: data.DTA æ–‡ä»¶")
        report_lines.append(f"   â€¢ ç›®æ ‡åˆ—: {CONFIG['target_column']}")
        report_lines.append(f"   â€¢ æ•°æ®æ€»æ•°: {len(self.data)}")
        report_lines.append(f"   â€¢ æœ‰æ•ˆåºåˆ—æ•°: {len(self.actual_values)}")
        report_lines.append(f"   â€¢ æ—¶é—´èŒƒå›´: {self.data.index[0]} åˆ° {self.data.index[-1]}")
        report_lines.append("")
        
        # æ¨¡åž‹æ€§èƒ½
        report_lines.append("ðŸ† æ¨¡åž‹æ€§èƒ½æŒ‡æ ‡:")
        report_lines.append(f"   â€¢ MSE (å‡æ–¹è¯¯å·®): {self.results['MSE']:.6f}")
        report_lines.append(f"   â€¢ MAE (å¹³å‡ç»å¯¹è¯¯å·®): {self.results['MAE']:.6f}")
        report_lines.append(f"   â€¢ RMSE (å‡æ–¹æ ¹è¯¯å·®): {self.results['RMSE']:.6f}")
        report_lines.append(f"   â€¢ RÂ² (å†³å®šç³»æ•°): {self.results['R2']:.6f}")
        report_lines.append(f"   â€¢ MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®): {self.results['MAPE']:.2f}%")
        report_lines.append(f"   â€¢ æ–¹å‘å‡†ç¡®çŽ‡: {self.results['Direction_Accuracy']:.2f}%")
        report_lines.append("")
        
        # æ€§èƒ½è¯„ä¼°
        report_lines.append("ðŸ“ˆ æ¨¡åž‹æ€§èƒ½è¯„ä¼°:")
        if self.results['R2'] > 0.8:
            report_lines.append("   âœ… æ¨¡åž‹æ€§èƒ½ä¼˜ç§€ï¼ˆRÂ² > 0.8ï¼‰ï¼Œé¢„æµ‹ç²¾åº¦é«˜")
        elif self.results['R2'] > 0.6:
            report_lines.append("   âœ“ æ¨¡åž‹æ€§èƒ½è‰¯å¥½ï¼ˆ0.6 < RÂ² â‰¤ 0.8ï¼‰ï¼Œé¢„æµ‹ç²¾åº¦è¾ƒå¥½")
        elif self.results['R2'] > 0.4:
            report_lines.append("   âš  æ¨¡åž‹æ€§èƒ½ä¸€èˆ¬ï¼ˆ0.4 < RÂ² â‰¤ 0.6ï¼‰ï¼Œéœ€è¦ä¼˜åŒ–")
        else:
            report_lines.append("   âŒ æ¨¡åž‹æ€§èƒ½è¾ƒå·®ï¼ˆRÂ² â‰¤ 0.4ï¼‰ï¼Œéœ€è¦é‡æ–°è°ƒæ•´")
        report_lines.append("")
        
        # æ•°æ®ç»Ÿè®¡
        report_lines.append("ðŸ“Š æ•°æ®ç»Ÿè®¡åˆ†æž:")
        report_lines.append(f"   å®žé™…å€¼:")
        report_lines.append(f"      â€¢ å¹³å‡å€¼: {self.actual_values.mean():.4f}")
        report_lines.append(f"      â€¢ æ ‡å‡†å·®: {self.actual_values.std():.4f}")
        report_lines.append(f"      â€¢ æœ€å°å€¼: {self.actual_values.min():.4f}")
        report_lines.append(f"      â€¢ æœ€å¤§å€¼: {self.actual_values.max():.4f}")
        report_lines.append(f"   é¢„æµ‹å€¼:")
        report_lines.append(f"      â€¢ å¹³å‡å€¼: {self.predictions.mean():.4f}")
        report_lines.append(f"      â€¢ æ ‡å‡†å·®: {self.predictions.std():.4f}")
        report_lines.append(f"      â€¢ æœ€å°å€¼: {self.predictions.min():.4f}")
        report_lines.append(f"      â€¢ æœ€å¤§å€¼: {self.predictions.max():.4f}")
        report_lines.append("")
        
        # æ¨¡åž‹é…ç½®
        report_lines.append("âš™ï¸ æ¨¡åž‹é…ç½®:")
        report_lines.append(f"   â€¢ åºåˆ—é•¿åº¦: {CONFIG['sequence_length']}")
        report_lines.append(f"   â€¢ LSTMå•å…ƒæ•°: {CONFIG['lstm_units']}")
        report_lines.append(f"   â€¢ Attentionç»´åº¦: {CONFIG['attention_dim']}")
        report_lines.append(f"   â€¢ Dropoutæ¯”çŽ‡: {CONFIG['dropout_rate']}")
        report_lines.append(f"   â€¢ å­¦ä¹ çŽ‡: {CONFIG['learning_rate']}")
        report_lines.append(f"   â€¢ è®­ç»ƒè½®æ•°: {CONFIG['epochs']}")
        report_lines.append(f"   â€¢ æ‰¹æ¬¡å¤§å°: {CONFIG['batch_size']}")
        report_lines.append(f"   â€¢ æµ‹è¯•é›†æ¯”ä¾‹: {CONFIG['test_size']}")
        report_lines.append("")
        
        # æ®‹å·®åˆ†æž
        residuals = self.actual_values - self.predictions
        report_lines.append("ðŸ” æ®‹å·®åˆ†æž:")
        report_lines.append(f"   â€¢ æ®‹å·®å‡å€¼: {residuals.mean():.6f}")
        report_lines.append(f"   â€¢ æ®‹å·®æ ‡å‡†å·®: {residuals.std():.6f}")
        report_lines.append(f"   â€¢ æ®‹å·®èŒƒå›´: [{residuals.min():.6f}, {residuals.max():.6f}]")
        report_lines.append("")
        
        # å»ºè®®
        report_lines.append("ðŸ’¡ åº”ç”¨å»ºè®®:")
        report_lines.append("   â€¢ æ¨¡åž‹ä½¿ç”¨LSTM + Attentionæœºåˆ¶ï¼Œå…·æœ‰è¾ƒå¼ºçš„æ—¶é—´åºåˆ—å»ºæ¨¡èƒ½åŠ›")
        report_lines.append("   â€¢ Attentionæœºåˆ¶èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ä¸åŒæ—¶é—´æ­¥çš„é‡è¦ç¨‹åº¦")
        report_lines.append("   â€¢ å»ºè®®å®šæœŸæ›´æ–°æ¨¡åž‹ä»¥é€‚åº”å¸‚åœºå˜åŒ–")
        report_lines.append("   â€¢ ç»“åˆå¸‚åœºåŸºæœ¬é¢è¿›è¡Œç»¼åˆåˆ¤æ–­")
        report_lines.append("   â€¢ ç›‘æŽ§å…³é”®ç‰¹å¾çš„å˜åŒ–è¶‹åŠ¿")
        report_lines.append("")
        
        # è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
        report_lines.append("ðŸ“ ç”Ÿæˆæ–‡ä»¶æ¸…å•:")
        report_lines.append(f"   â€¢ ExcelæŠ¥å‘Š: {os.path.join('outputs/reports', f'{self.run_timestamp}_carbon_prediction_report.xlsx')}")
        report_lines.append(f"   â€¢ è®­ç»ƒåŽ†å²å›¾: {os.path.join('outputs/visualizations', f'{self.run_timestamp}_training_history.png')}")
        report_lines.append(f"   â€¢ é¢„æµ‹å¯¹æ¯”å›¾: {os.path.join('outputs/visualizations', f'{self.run_timestamp}_predictions.png')}")
        report_lines.append(f"   â€¢ æ®‹å·®åˆ†æžå›¾: {os.path.join('outputs/visualizations', f'{self.run_timestamp}_residuals.png')}")
        report_lines.append("")
        
        report_lines.append("=" * 80)
        report_lines.append("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        report_lines.append("=" * 80)
        
        # ä¿å­˜æ–‡ä»¶
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"âœ… TXTæŠ¥å‘Šå·²ä¿å­˜åˆ°: {txt_path}")
        return txt_path
    
    def run_complete_analysis(self):
        """å®Œæ•´åˆ†æžæµç¨‹"""
        print("\n" + "="*80)
        print(" " * 20 + "LSTM + Attention Carbon Price Prediction System")
        print("="*80 + "\n")
        
        try:
            # 1. Load Data
            self.load_data(CONFIG['data_file'])
            
            # 2. Data Preprocessing
            self.preprocess_data()
            
            # 3. Split and Scale Data
            X_train, y_train, X_val, y_val, X_test, y_test = self.split_and_scale_data()
            
            # 4. Train Model
            self.train_model(X_train, y_train, X_val, y_val)
            
            # 5. Evaluate Model
            self.evaluate_model(X_test, y_test)
            
            # 6. SHAP Analysis (ä½¿ç”¨åŽŸå§‹æ•°æ®è®­ç»ƒRFæ¨¡åž‹)
            if SHAP_AVAILABLE:
                # å‡†å¤‡ç”¨äºŽSHAPçš„æ•°æ®ï¼ˆä¸ä½¿ç”¨åºåˆ—ï¼‰
                df = self.processed_data
                target = CONFIG['target_column']
                n = len(df)
                train_size = int(n * (1 - CONFIG['test_size']))
                
                X_train_ml = df[self.feature_names].iloc[:train_size].values
                y_train_ml = df[target].iloc[:train_size].values
                X_test_ml = df[self.feature_names].iloc[train_size:].values
                
                # å¤„ç†NaN
                X_train_ml = np.nan_to_num(X_train_ml, nan=0.0)
                y_train_ml = np.nan_to_num(y_train_ml, nan=0.0)
                X_test_ml = np.nan_to_num(X_test_ml, nan=0.0)
                
                self.perform_shap_analysis(X_train_ml, y_train_ml, X_test_ml)
            
            # 7. Create Visualizations
            self.create_visualizations()
            
            # 8. Generate Excel Report
            excel_path = self.generate_excel_report()
            
            # 9. Generate TXT Report
            txt_path = self.generate_text_report()
            
            print("\n" + "="*80)
            print("âœ… Complete Analysis Finished Successfully!")
            print("="*80)
            print("\nðŸ“ Generated Files:")
            print(f"   â€¢ Excel Report: {excel_path}")
            print(f"   â€¢ TXT Report: {txt_path}")
            print(f"   â€¢ Visualizations: outputs/visualizations/")
            if SHAP_AVAILABLE and self.shap_values is not None:
                print(f"   â€¢ SHAP Analysis: Completed")
            print("\n" + "="*80 + "\n")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ Error: {str(e)}")
            import traceback
            traceback.print_exc()
            return False


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

if __name__ == '__main__':
    system = LSTMAttentionCarbonPrediction()
    system.run_complete_analysis()
